{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example script for training MPNN-POM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "from tqdm import tqdm\n",
    "# from openpom.feat.graph_featurizer import GraphFeaturizer, GraphConvConstants\n",
    "# from openpom.utils.data_utils import get_class_imbalance_ratio\n",
    "# from openpom.models.mpnn_pom import MPNNPOMModel\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.models.optimizers import Optimizer\n",
    "from deepchem.models.optimizers import Adam\n",
    "from deepchem.models.optimizers import AdaGrad\n",
    "from deepchem.models.optimizers import AdamW\n",
    "from deepchem.models.optimizers import SparseAdam\n",
    "from deepchem.models.optimizers import RMSProp\n",
    "from deepchem.models.optimizers import GradientDescent\n",
    "from deepchem.models.optimizers import KFAC\n",
    "\n",
    "\n",
    "def get_optimizer(optimizer_name: str = 'adam') -> Optimizer:\n",
    "    \"\"\"\n",
    "    Get deepchem optimizer object\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    optimizer_name: str\n",
    "      optimizer name\n",
    "      choices: [adam, adagrad, adamw, sparseadam, rmsprop, sgd, kfac]\n",
    "      default: 'adam'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Optimizer\n",
    "      Deepchem optimizer object\n",
    "    \"\"\"\n",
    "    if optimizer_name == 'adam':\n",
    "        return Adam()\n",
    "    elif optimizer_name == 'adagrad':\n",
    "        return AdaGrad()\n",
    "    elif optimizer_name == 'adamw':\n",
    "        return AdamW()\n",
    "    elif optimizer_name == 'sparseadam':\n",
    "        return SparseAdam()\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        return RMSProp()\n",
    "    elif optimizer_name == 'sgd':\n",
    "        return GradientDescent()\n",
    "    elif optimizer_name == 'kfac':\n",
    "        return KFAC()\n",
    "    else:\n",
    "        print(\"INVALID OPTIMISER NAME!, using ADAM optimizer by default\")\n",
    "        return Adam()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Molecule_Feature_Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from deepchem.utils.typing import RDKitAtom\n",
    "from deepchem.utils.molecule_feature_utils import one_hot_encode\n",
    "\n",
    "\n",
    "def get_atomic_num_one_hot(atom: RDKitAtom,\n",
    "                           allowable_set: List[int],\n",
    "                           include_unknown_set: bool = True) -> List[float]:\n",
    "    \"\"\"\n",
    "    Get a one-hot feature about atomic number of the given atom.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    atom: RDKitAtom\n",
    "        RDKit atom object\n",
    "    allowable_set: List[int]\n",
    "        The range of atomic numbers to consider.\n",
    "    include_unknown_set: bool, default False\n",
    "        If true, the index of all types not in\n",
    "        `allowable_set` is `len(allowable_set)`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A one-hot vector of atomic number of the given atom.\n",
    "        If `include_unknown_set` is False, the length is\n",
    "        `len(allowable_set)`.\n",
    "        If `include_unknown_set` is True, the length is\n",
    "        `len(allowable_set) + 1`.\n",
    "\n",
    "    \"\"\"\n",
    "    return one_hot_encode(atom.GetAtomicNum() - 1, allowable_set,\n",
    "                          include_unknown_set)\n",
    "\n",
    "\n",
    "def get_atom_total_valence_one_hot(\n",
    "        atom: RDKitAtom,\n",
    "        allowable_set: List[int],\n",
    "        include_unknown_set: bool = True) -> List[float]:\n",
    "    \"\"\"Get a one-hot feature for total valence of an atom.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    atom: rdkit.Chem.rdchem.Atom\n",
    "        RDKit atom object\n",
    "    allowable_set: List[int]\n",
    "        Atom total valence to consider.\n",
    "    include_unknown_set: bool, default True\n",
    "        If true, the index of all types not in\n",
    "        `allowable_set` is `len(allowable_set)`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A one-hot vector for total valence an atom has.\n",
    "        If `include_unknown_set` is False, the length is\n",
    "        `len(allowable_set)`.\n",
    "        If `include_unknown_set` is True, the length is\n",
    "        `len(allowable_set) + 1`.\n",
    "\n",
    "    \"\"\"\n",
    "    return one_hot_encode(atom.GetTotalValence(), allowable_set,\n",
    "                          include_unknown_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional, Callable, List\n",
    "from deepchem.models.losses import Loss\n",
    "\n",
    "class CustomMultiLabelLoss(Loss):\n",
    "    \"\"\"\n",
    "    Custom Multi-Label Loss function for multi-label classification.\n",
    "\n",
    "    The objective function is a summed cross-entropy loss over all tasks,\n",
    "    with each task's contribution to the loss being weighted by a factor\n",
    "    of log(1+ class_imbalance_ratio), such that rarer tasks were given\n",
    "    a higher weighting.\n",
    "\n",
    "    This loss function is based on:\n",
    "    `A Principal Odor Map Unifies Diverse Tasks in Human Olfactory Perception\n",
    "    preprint <https://www.biorxiv.org/content/10.1101/2022.09.01.504602v4>`_.\n",
    "\n",
    "    The labels should have shape (batch_size) or (batch_size, tasks), and be\n",
    "    integer class labels. The outputs have shape (batch_size, classes) or\n",
    "    (batch_size, tasks, classes) and be logits that are converted to\n",
    "    probabilities using a softmax function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 class_imbalance_ratio: Optional[List] = None,\n",
    "                 loss_aggr_type: str = 'sum',\n",
    "                 sample_weights: Optional[List[float]] = None,\n",
    "                 device: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        class_imbalance_ratio: Optional[List]\n",
    "            List of class imbalance ratios.\n",
    "        loss_aggr_type: str\n",
    "            Loss aggregation type; 'sum' or 'mean'.\n",
    "        sample_weights: Optional[List[float]]\n",
    "            List of sample weights for each data point.\n",
    "        device: Optional[str]\n",
    "            The device on which to run computations. If None, a device is\n",
    "            chosen automatically.\n",
    "        \"\"\"\n",
    "        super(CustomMultiLabelLoss, self).__init__()\n",
    "        if class_imbalance_ratio is None:\n",
    "            print(Warning(\"No class imbalance ratio provided!\"))\n",
    "            self.class_imbalance_ratio: Optional[torch.Tensor] = None\n",
    "        else:\n",
    "            self.class_imbalance_ratio = torch.Tensor(class_imbalance_ratio)\n",
    "\n",
    "        self.sample_weights: Optional[torch.Tensor] = None\n",
    "        if sample_weights is not None:\n",
    "            self.sample_weights = torch.Tensor(sample_weights)\n",
    "\n",
    "        if loss_aggr_type not in ['sum', 'mean']:\n",
    "            raise ValueError(f\"Invalid loss aggregate type: {loss_aggr_type}\")\n",
    "        self.loss_aggr_type: str = loss_aggr_type\n",
    "\n",
    "        if device is not None:\n",
    "            if self.class_imbalance_ratio is not None:\n",
    "                self.class_imbalance_ratio = self.class_imbalance_ratio.to(device)\n",
    "            if self.sample_weights is not None:\n",
    "                self.sample_weights = self.sample_weights.to(device)\n",
    "\n",
    "    def _create_pytorch_loss(self) -> Callable[[torch.Tensor, torch.Tensor], torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns loss function for pytorch backend\n",
    "        \"\"\"\n",
    "        ce_loss_fn: torch.nn.CrossEntropyLoss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "        def loss(output: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"\n",
    "            The objective function is a summed cross-entropy loss over all\n",
    "            tasks, with each task's contribution to the loss being weighted\n",
    "            by a factor of log(1+ class_imbalance_ratio), such that rarer\n",
    "            tasks were given a higher weighting.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            output: torch.Tensor\n",
    "                Output logits from model's forward pass per batch.\n",
    "            labels: torch.Tensor\n",
    "                Target labels per batch.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            loss: torch.Tensor\n",
    "                Total or mean loss depending on loss aggregation type.\n",
    "            \"\"\"\n",
    "            # Convert (batch_size, tasks, classes) to (batch_size, classes, tasks)\n",
    "            if len(output.shape) == 3:\n",
    "                output = output.permute(0, 2, 1)\n",
    "\n",
    "            if len(labels.shape) == len(output.shape):\n",
    "                labels = labels.squeeze(-1)\n",
    "\n",
    "            # Handle multilabel\n",
    "            probabilities: torch.Tensor = output[:, 0, :]\n",
    "            complement_probabilities: torch.Tensor = 1 - probabilities\n",
    "            binary_output: torch.Tensor = torch.stack([complement_probabilities, probabilities], dim=1)\n",
    "\n",
    "            ce_loss: torch.Tensor = ce_loss_fn(binary_output, labels.long())\n",
    "\n",
    "            if self.class_imbalance_ratio is None:\n",
    "                if self.loss_aggr_type == 'sum':\n",
    "                    loss: torch.Tensor = ce_loss.sum(dim=1)\n",
    "                else:\n",
    "                    loss = ce_loss.mean(dim=1)\n",
    "            else:\n",
    "                balancing_factors: torch.Tensor = torch.log(1 + self.class_imbalance_ratio)\n",
    "                balanced_losses: torch.Tensor = torch.mul(ce_loss, balancing_factors)\n",
    "\n",
    "                if self.loss_aggr_type == 'sum':\n",
    "                    loss = balanced_losses.sum(dim=1)\n",
    "                else:\n",
    "                    loss = balanced_losses.mean(dim=1)\n",
    "\n",
    "            if self.sample_weights is not None:\n",
    "                batch_sample_weights = self.sample_weights[:loss.size(0)].to(loss.device)\n",
    "                loss = loss * batch_sample_weights\n",
    "\n",
    "            return loss.unsqueeze(-1).repeat(1, output.shape[-1])\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data_Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple, Iterator\n",
    "from deepchem.data.datasets import DiskDataset, NumpyDataset\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from deepchem.splits import Splitter\n",
    "\n",
    "\n",
    "def get_class_imbalance_ratio(dataset: DiskDataset) -> List:\n",
    "    \"\"\"\n",
    "    Get imbalance ratio per task from DiskDataset\n",
    "\n",
    "    Imbalance ratio per label (IRLbl): Let M be an MLD with a set of\n",
    "    labels L and Yi be the label-set of the ith instance. IRLbl is calcu-\n",
    "    lated for the label λ as the ratio between the majority label and\n",
    "    the label λ, where IRLbl is 1 for the most frequent label and a\n",
    "    greater value for the rest. The larger the value of IRLbl, the higher\n",
    "    the imbalance level for the concerned label.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    dataset: DiskDataset\n",
    "        Deepchem diskdataset object to get class imbalance ratio\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    class_imbalance_ratio: List\n",
    "        List of imbalance ratios per task\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. TarekegnA.N. et al.\n",
    "       \"A review of methods for imbalanced multi-label classification\"\n",
    "       Pattern Recognit. (2021)\n",
    "    \"\"\"\n",
    "    if not isinstance(dataset, DiskDataset) and not isinstance(dataset, NumpyDataset):\n",
    "        raise Exception(\"The dataset should be a deepchem DiskDataset or NumpyDataset\")\n",
    "    df: pd.DataFrame = pd.DataFrame(dataset.y)\n",
    "    class_counts: np.ndarray = df.sum().to_numpy()\n",
    "    max_count: int = max(class_counts)\n",
    "    class_imbalance_ratio: List = (class_counts / max_count).tolist()\n",
    "    return class_imbalance_ratio\n",
    "\n",
    "\n",
    "class IterativeStratifiedSplitter(Splitter):\n",
    "    \"\"\"\n",
    "    Iteratively stratify a multi-label data set into folds/splits.\n",
    "\n",
    "    Construct an iterative stratifier that splits the dataset\n",
    "    trying to maintain balanced representation with respect to\n",
    "    order-th label combinations.\n",
    "\n",
    "    Available splits:\n",
    "        - train_valid_test_split()\n",
    "        - train_test_split()\n",
    "\n",
    "    Note:\n",
    "        Requires `skmultilearn` library to be installed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, order: int = 2) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ---------\n",
    "        order: int\n",
    "            order for iterative stratification (default: 2)\n",
    "        \"\"\"\n",
    "        self.order: int = order\n",
    "\n",
    "    def split(\n",
    "        self,\n",
    "        dataset: DiskDataset,\n",
    "        frac_train: float = 0.8,\n",
    "        frac_valid: float = 0.1,\n",
    "        frac_test: float = 0.1,\n",
    "        seed: Optional[int] = None,\n",
    "        log_every_n: Optional[int] = None\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Return indices for iterative stratified split\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: dc.data.Dataset\n",
    "            Dataset to be split.\n",
    "        seed: int, optional (default None)\n",
    "            Random seed to use.\n",
    "        frac_train: float, optional (default 0.8)\n",
    "            The fraction of data to be used for the training split.\n",
    "        frac_valid: float, optional (default 0.1)\n",
    "            The fraction of data to be used for the validation split.\n",
    "        frac_test: float, optional (default 0.1)\n",
    "            The fraction of data to be used for the test split.\n",
    "        log_every_n: int, optional (default None)\n",
    "            Controls the logger by dictating how often logger outputs\n",
    "            will be produced.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "            A tuple `(train_indices, valid_indices, test_indices)`\n",
    "            for the various splits.\n",
    "        \"\"\"\n",
    "        X1: pd.DataFrame\n",
    "        y1: pd.DataFrame\n",
    "        X1, y1 = pd.DataFrame(dataset.X), pd.DataFrame(dataset.y)\n",
    "        stratifier1: IterativeStratification = IterativeStratification(\n",
    "            n_splits=2,\n",
    "            order=self.order,\n",
    "            sample_distribution_per_fold=[frac_test + frac_valid, frac_train],\n",
    "            # shuffle=True,\n",
    "            random_state=seed,\n",
    "        )\n",
    "\n",
    "        train_indices: np.ndarray\n",
    "        other_indices: np.ndarray\n",
    "        train_indices, other_indices = next(stratifier1.split(X1, y1))\n",
    "\n",
    "        temp_dir: str = tempfile.mkdtemp()\n",
    "        other_dataset: DiskDataset = dataset.select(other_indices.tolist(),\n",
    "                                                    temp_dir)\n",
    "\n",
    "        X2: pd.DataFrame\n",
    "        y2: pd.DataFrame\n",
    "        X2, y2 = pd.DataFrame(other_dataset.X), pd.DataFrame(other_dataset.y)\n",
    "        new_split_ratio: float = round(frac_test / (frac_test + frac_valid), 2)\n",
    "        stratifier2: IterativeStratification = IterativeStratification(\n",
    "            n_splits=2,\n",
    "            order=self.order,\n",
    "            sample_distribution_per_fold=[\n",
    "                new_split_ratio, 1 - new_split_ratio\n",
    "            ],\n",
    "            random_state=seed,\n",
    "        )\n",
    "\n",
    "        valid_indices: np.ndarray\n",
    "        test_indices: np.ndarray\n",
    "        valid_indices, test_indices = next(stratifier2.split(X2, y2))\n",
    "        return train_indices, valid_indices, test_indices\n",
    "\n",
    "    def k_fold_split(\n",
    "        self,\n",
    "        dataset: DiskDataset,\n",
    "        k: int,\n",
    "        directories: Optional[List[str]] = None\n",
    "    ) -> List[Tuple[DiskDataset, DiskDataset]]:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: DiskDataset\n",
    "            DiskDataset to do a k-fold split\n",
    "        k: int\n",
    "            Number of folds to split `DiskDataset` into. (k>1)\n",
    "        directories: List[str], optional (default None)\n",
    "            List of length 2*k filepaths to save the result disk-datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[Tuple[DiskDataset, DiskDataset]]\n",
    "            List of length k tuples of (train, cv)\n",
    "            where `train` and `cv` are both `DiskDataset`.\n",
    "        \"\"\"\n",
    "        assert k != 1\n",
    "        if directories is None:\n",
    "            directories = [tempfile.mkdtemp() for _ in range(2 * k)]\n",
    "        else:\n",
    "            assert len(directories) == 2 * k\n",
    "\n",
    "        X: pd.DataFrame\n",
    "        y: pd.DataFrame\n",
    "        X, y = pd.DataFrame(dataset.X), pd.DataFrame(dataset.y)\n",
    "        stratifier: IterativeStratification = IterativeStratification(\n",
    "            n_splits=k,\n",
    "            order=self.order,\n",
    "        )\n",
    "\n",
    "        train_datasets: List = []\n",
    "        cv_datasets: List = []\n",
    "        split_gen: Iterator = stratifier.split(X, y)\n",
    "        for fold in range(k):\n",
    "            train_dir, cv_dir = directories[2 * fold], directories[2 * fold +\n",
    "                                                                   1]\n",
    "            train_indices: np.ndarray\n",
    "            cv_indices: np.ndarray\n",
    "            train_indices, cv_indices = next(split_gen)\n",
    "            train_dataset: DiskDataset = dataset.select(\n",
    "                train_indices.tolist(), train_dir)\n",
    "            cv_dataset: DiskDataset = dataset.select(cv_indices.tolist(),\n",
    "                                                     cv_dir)\n",
    "            train_datasets.append(train_dataset)\n",
    "            cv_datasets.append(cv_dataset)\n",
    "        return list(zip(train_datasets, cv_datasets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_weights(dataset: DiskDataset, intersection: set) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate random sample weights for each sample in the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: DiskDataset\n",
    "        Deepchem diskdataset object to get sample weights.\n",
    "    seed: Optional[int]\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sample_weights: List[float]\n",
    "        List of random weights for each sample in the dataset.\n",
    "    \"\"\"\n",
    "    sample_weights = []\n",
    "    for elt in dataset.X[:,1]:\n",
    "        # if the smiles is in the challenge intersection change sample weight to 2\n",
    "        if elt in intersection:\n",
    "            sample_weights.append(2)\n",
    "        else:\n",
    "            sample_weights.append(1)\n",
    "    return sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from rdkit import Chem\n",
    "from typing import List, Union, Dict, Sequence\n",
    "from deepchem.utils.typing import RDKitAtom, RDKitBond, RDKitMol\n",
    "from deepchem.feat.base_classes import MolecularFeaturizer\n",
    "from deepchem.feat.graph_data import GraphData\n",
    "from deepchem.utils.molecule_feature_utils import get_atom_total_degree_one_hot\n",
    "from deepchem.utils.molecule_feature_utils \\\n",
    "    import get_atom_formal_charge_one_hot\n",
    "from deepchem.utils.molecule_feature_utils \\\n",
    "    import get_atom_total_num_Hs_one_hot\n",
    "from deepchem.utils.molecule_feature_utils \\\n",
    "    import get_atom_hybridization_one_hot\n",
    "# from openpom.utils.molecule_feature_utils \\\n",
    "#     import get_atomic_num_one_hot, get_atom_total_valence_one_hot\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# get the smile in \n",
    "def smilesAtomReorder(atoms,smiles):\n",
    "    atoms_reorder = []\n",
    "    i = 0\n",
    "    while i < len(smiles):\n",
    "        # handle 2 letters atoms\n",
    "        if i + 2 <= len(smiles):\n",
    "            symbol2 = smiles[i:i+2]\n",
    "            len2App = False\n",
    "            if symbol2.isalpha():\n",
    "                for atom in atoms:\n",
    "                    if atom.GetSymbol() == symbol2:\n",
    "                        atoms_reorder.append(atom)\n",
    "                        atoms.remove(atom)\n",
    "                        i+=2\n",
    "                        len2App = True\n",
    "                        break\n",
    "            if len2App:\n",
    "                continue\n",
    "        # append single letter atoms\n",
    "        symbol = smiles[i]\n",
    "        if symbol.isalpha():\n",
    "            for atom in atoms:\n",
    "                if atom.GetSymbol() == symbol or atom.GetSymbol().lower()==symbol:\n",
    "                    atoms.remove(atom)\n",
    "                    atoms_reorder.append(atom)\n",
    "                    break\n",
    "        i+=1\n",
    "                \n",
    "    return atoms_reorder\n",
    "                    \n",
    "\n",
    "class GraphConvConstants(object):\n",
    "    \"\"\"\n",
    "    A class for holding featurization parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    MAX_ATOMIC_NUM = 100\n",
    "    ATOM_FEATURES: Dict[str, List[int]] = {\n",
    "        'valence': [0, 1, 2, 3, 4, 5, 6],\n",
    "        'degree': [0, 1, 2, 3, 4, 5],\n",
    "        'num_Hs': [0, 1, 2, 3, 4],\n",
    "        'formal_charge': [-1, -2, 1, 2, 0],\n",
    "        'atomic_num': list(range(MAX_ATOMIC_NUM)),\n",
    "    }\n",
    "    ATOM_FEATURES_HYBRIDIZATION: List[str] = [\n",
    "        \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\"\n",
    "    ]\n",
    "    # 3D Coordinate Information\n",
    "\n",
    "    # Dimension of atom feature vector + 3D coordinate info\n",
    "    ATOM_FDIM = sum(len(choices) + 1\n",
    "                    for choices in ATOM_FEATURES.values()) + len(\n",
    "                        ATOM_FEATURES_HYBRIDIZATION) + 1\n",
    "    # len(choices) +1 and len(ATOM_FEATURES_HYBRIDIZATION)\n",
    "    # + 1 to include room for unknown set\n",
    "    BOND_FDIM = 6\n",
    "\n",
    "\n",
    "def atom_features(atom: RDKitAtom) -> Sequence[Union[bool, int, float]]:\n",
    "    \"\"\"\n",
    "    Helper method used to compute atom feature vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    atom: RDKitAtom\n",
    "        Atom to compute features on.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features: Sequence[Union[bool, int, float]]\n",
    "        A list of atom features.\n",
    "    \"\"\"\n",
    "    if atom is None:\n",
    "        features: Sequence[Union[bool, int,\n",
    "                                 float]] = [0] * GraphConvConstants.ATOM_FDIM\n",
    "\n",
    "    else:\n",
    "        features = []\n",
    "        features += get_atom_total_valence_one_hot(\n",
    "            atom, GraphConvConstants.ATOM_FEATURES['valence'])\n",
    "        features += get_atom_total_degree_one_hot(\n",
    "            atom, GraphConvConstants.ATOM_FEATURES['degree'])\n",
    "        features += get_atom_total_num_Hs_one_hot(\n",
    "            atom, GraphConvConstants.ATOM_FEATURES['num_Hs'])\n",
    "        features += get_atom_formal_charge_one_hot(\n",
    "            atom, GraphConvConstants.ATOM_FEATURES['formal_charge'])\n",
    "        features += get_atomic_num_one_hot(\n",
    "            atom, GraphConvConstants.ATOM_FEATURES['atomic_num'])\n",
    "        features += get_atom_hybridization_one_hot(\n",
    "            atom, GraphConvConstants.ATOM_FEATURES_HYBRIDIZATION, True)\n",
    "        features = [int(feature) for feature in features]\n",
    "        # Add 3D information at the end\n",
    "        # features += getAtom3dInfo(smiles,idx)\n",
    "        # features += [0,0,0]\n",
    "    return features\n",
    "\n",
    "\n",
    "def bond_features(bond: RDKitBond) -> Sequence[Union[bool, int, float]]:\n",
    "    \"\"\"\n",
    "    Helper method used to compute bond feature vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bond: RDKitBond\n",
    "        Bond to compute features on.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features: Sequence[Union[bool, int, float]]\n",
    "        A list of bond features.\n",
    "    \"\"\"\n",
    "    if bond is None:\n",
    "        b_features: Sequence[Union[\n",
    "            bool, int, float]] = [1] + [0] * (GraphConvConstants.BOND_FDIM - 1)\n",
    "\n",
    "    else:\n",
    "        bt = bond.GetBondType()\n",
    "        b_features = [\n",
    "            0, bt == Chem.rdchem.BondType.SINGLE,\n",
    "            bt == Chem.rdchem.BondType.DOUBLE,\n",
    "            bt == Chem.rdchem.BondType.TRIPLE,\n",
    "            bt == Chem.rdchem.BondType.AROMATIC,\n",
    "            bond.IsInRing()\n",
    "        ]\n",
    "\n",
    "    return b_features\n",
    "\n",
    "\n",
    "class GraphFeaturizer(MolecularFeaturizer):\n",
    "    \"\"\"\n",
    "    This class is a featurizer for GNN (MESSAGE PASSING) implementation for\n",
    "    Principal Odor Map.\n",
    "\n",
    "    The default node(atom) and edge(bond) representations are based on\n",
    "    `A Principal Odor Map Unifies Diverse Tasks in Human Olfactory Perception\n",
    "    preprint <https://www.biorxiv.org/content/10.1101/2022.09.01.504602v4>`_.\n",
    "\n",
    "    The default node representation are constructed by concatenating\n",
    "    the following values, and the feature length is 134.\n",
    "\n",
    "    - Valence: A one-hot vector for total valence (0-6) of an atom.\n",
    "    - Degree: A one-hot vector of the degree (0-5) of this atom.\n",
    "    - Number of Hydrogens: A one-hot vector of the number of hydrogens\n",
    "      (0-4) that this atom connected.\n",
    "    - Formal charge: Integer electronic charge, -1, -2, 1, 2, 0.\n",
    "    - Atomic num: A one-hot vector of this atom, in a range of first 100 atoms.\n",
    "    - Hybridization: A one-hot vector of \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\".\n",
    "\n",
    "    The default edge representation are constructed by concatenating\n",
    "    the following values, and the feature length is 6.\n",
    "\n",
    "    - Bond type: A one-hot vector of the bond type,\n",
    "      \"single\", \"double\", \"triple\", or \"aromatic\".\n",
    "    - Is in ring: Boolean value to specify whether\n",
    "      the bond is in a ring or not.\n",
    "\n",
    "    If you want to know more details about features,\n",
    "    please check the paper [1]_ and utilities in\n",
    "    deepchem.utils.molecule_feature_utils.py.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Kearnes, Steven, et al.\n",
    "       \"Molecular graph convolutions: moving beyond fingerprints.\"\n",
    "        Journal of computer-aided molecular design 30.8 (2016):595-608.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    This class requires RDKit to be installed.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, is_adding_hs=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        is_adding_hs: bool, default False\n",
    "            Whether to add Hs or not.\n",
    "        \"\"\"\n",
    "        self.is_adding_hs = is_adding_hs\n",
    "        super(GraphFeaturizer).__init__()\n",
    "\n",
    "    def _construct_bond_index(self, datapoint: RDKitMol) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Construct edge (bond) index\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        datapoint: RDKitMol\n",
    "            RDKit mol object.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        edge_index: np.ndarray\n",
    "            Edge (Bond) index\n",
    "\n",
    "        \"\"\"\n",
    "        src: List[int] = []\n",
    "        dest: List[int] = []\n",
    "        for bond in datapoint.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            src += [start, end]\n",
    "            dest += [end, start]\n",
    "        return np.asarray([src, dest], dtype=int)\n",
    "\n",
    "    def _featurize(self, datapoint: RDKitMol, **kwargs) -> GraphData:\n",
    "        \"\"\"Calculate molecule graph features from RDKit mol object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        datapoint: RDKitMol\n",
    "            RDKit mol object.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        graph: GraphData\n",
    "            A molecule graph object with features:\n",
    "            - node_features: Node feature matrix with shape\n",
    "              [num_nodes, num_node_features]\n",
    "            - edge_index: Graph connectivity in COO format with shape\n",
    "              [2, num_edges]\n",
    "            - edge_features: Edge feature matrix with shape\n",
    "              [num_edges, num_edge_features]\n",
    "        \"\"\"\n",
    "        if isinstance(datapoint, Chem.rdchem.Mol):\n",
    "            if self.is_adding_hs:\n",
    "                datapoint = Chem.AddHs(datapoint)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Feature field should contain smiles for featurizer!\")\n",
    "\n",
    "        # get atom features\n",
    "        smiles = Chem.MolToSmiles(datapoint)\n",
    "        # atoms = list(datapoint.GetAtoms())\n",
    "\n",
    "        # atoms_reorder = smilesAtomReorder(atoms,smiles)\n",
    "        \n",
    "        # atoms_reorder = smilesAtomReorder(atoms,Chem.MolToSmiles(datapoint))\n",
    "        f_atoms: np.ndarray = np.asarray(\n",
    "            [atom_features(atom) for atom in datapoint.GetAtoms()],\n",
    "            dtype=float)\n",
    "        # print(smiles)\n",
    "        # for atom in atoms_reorder:\n",
    "        #     print(atom.GetSymbol())\n",
    "        \n",
    "        # get edge(bond) features\n",
    "        if len(datapoint.GetBonds()) == 0:\n",
    "            f_bonds: np.ndarray = np.empty((0, GraphConvConstants.BOND_FDIM))\n",
    "        else:\n",
    "            f_bonds_list = []\n",
    "            for bond in datapoint.GetBonds():\n",
    "                b_feat = 2 * [bond_features(bond)]\n",
    "                f_bonds_list.extend(b_feat)\n",
    "            f_bonds = np.asarray(f_bonds_list, dtype=float)\n",
    "\n",
    "        # get edge index\n",
    "        edge_index: np.ndarray = self._construct_bond_index(datapoint)\n",
    "\n",
    "        return (GraphData(node_features=f_atoms,\n",
    "                         edge_index=edge_index,\n",
    "                         edge_features=f_bonds),Chem.MolToSmiles(datapoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pom_ffn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional, Callable, Any\n",
    "\n",
    "\n",
    "class CustomPositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Customised PositionwiseFeedForward layer from deepchem\n",
    "    for:\n",
    "        - hidden layers of variable sizes\n",
    "        - batch normalization before every activation function\n",
    "        - additional output of embedding layer (penultimate layer)\n",
    "          for POM embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_input: int = 1024,\n",
    "        d_hidden_list: List = [1024],\n",
    "        d_output: int = 1024,\n",
    "        activation: str = 'leakyrelu',\n",
    "        dropout_p: float = 0.0,\n",
    "        dropout_at_input_no_act: bool = False,\n",
    "        batch_norm: bool = True,\n",
    "    ):\n",
    "        \"\"\"Initialize a PositionwiseFeedForward layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        d_input: int\n",
    "            Size of input layer.\n",
    "        d_hidden_list: List\n",
    "            List of hidden sizes.\n",
    "        d_output: int (same as d_input if d_output = 0)\n",
    "            Size of output layer.\n",
    "        activation: str\n",
    "            Activation function to be used. Can choose between 'relu' for ReLU,\n",
    "            'leakyrelu' for LeakyReLU, 'prelu' for PReLU,\n",
    "            'tanh' for TanH, 'selu' for SELU, 'elu' for ELU\n",
    "            and 'linear' for linear activation.\n",
    "        dropout_p: float\n",
    "            Dropout probability.\n",
    "        dropout_at_input_no_act: bool\n",
    "            If true, dropout is applied on the input tensor.\n",
    "            For single layer, it is not passed to an activation function.\n",
    "        batch_norm: bool\n",
    "            If true, applies batch normalization\n",
    "            'before' every activation function\n",
    "        \"\"\"\n",
    "        super(CustomPositionwiseFeedForward, self).__init__()\n",
    "\n",
    "        self.dropout_at_input_no_act: bool = dropout_at_input_no_act\n",
    "        self.batch_norm: bool = batch_norm\n",
    "\n",
    "        self.activation: Callable[[Any], Any]\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "\n",
    "        elif activation == 'leakyrelu':\n",
    "            self.activation = nn.LeakyReLU(0.1)\n",
    "\n",
    "        elif activation == 'prelu':\n",
    "            self.activation = nn.PReLU()\n",
    "\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "\n",
    "        elif activation == 'selu':\n",
    "            self.activation = nn.SELU()\n",
    "\n",
    "        elif activation == 'elu':\n",
    "            self.activation = nn.ELU()\n",
    "\n",
    "        elif activation == 'linear':\n",
    "            self.activation = lambda x: x\n",
    "\n",
    "        d_output = d_output if d_output != 0 else d_input\n",
    "\n",
    "        # Set n_layers\n",
    "        self.n_layers: int = len(d_hidden_list) + 1\n",
    "\n",
    "        # Set linear layers\n",
    "        if self.n_layers == 1:\n",
    "            linears: List = [nn.Linear(d_input, d_output)]\n",
    "\n",
    "        else:\n",
    "            linears = [nn.Linear(d_input, d_hidden_list[0])]\n",
    "            for idx in range(1, len(d_hidden_list)):\n",
    "                linears.append(\n",
    "                    nn.Linear(d_hidden_list[idx - 1], d_hidden_list[idx]))\n",
    "            linears.append(nn.Linear(d_hidden_list[-1], d_output))\n",
    "\n",
    "        self.linears: nn.ModuleList = nn.ModuleList(linears)\n",
    "        dropout_layer: nn.Dropout = nn.Dropout(dropout_p)\n",
    "        self.dropout_p: nn.ModuleList = nn.ModuleList(\n",
    "            [dropout_layer for _ in range(self.n_layers)])\n",
    "\n",
    "        if batch_norm:\n",
    "            batchnorms: List = [\n",
    "                nn.BatchNorm1d(d_hidden_list[idx])\n",
    "                for idx in range(len(d_hidden_list))\n",
    "            ]\n",
    "            self.batchnorms: nn.ModuleList = nn.ModuleList(batchnorms)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> List[Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Output Computation for the Customised\n",
    "        PositionwiseFeedForward layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            Input tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[Optional[torch.Tensor]]\n",
    "            List containing embeddings and output\n",
    "        \"\"\"\n",
    "\n",
    "        if self.n_layers == 1:\n",
    "            if self.dropout_at_input_no_act:\n",
    "                return [None, self.linears[0](self.dropout_p[0](x))]\n",
    "            else:\n",
    "                return [\n",
    "                    None,\n",
    "                    self.dropout_p[0](self.activation(self.linears[0](x)))\n",
    "                ]\n",
    "\n",
    "        else:\n",
    "            if self.dropout_at_input_no_act:\n",
    "                x = self.dropout_p[-1](x)\n",
    "\n",
    "            if self.batch_norm:\n",
    "                for i in range(self.n_layers - 2):\n",
    "                    x = self.dropout_p[i](self.activation(self.batchnorms[i](\n",
    "                        self.linears[i](x))))\n",
    "\n",
    "                embeddings: torch.Tensor = self.linears[self.n_layers - 2](x)\n",
    "                x = self.dropout_p[self.n_layers - 2](self.activation(\n",
    "                    self.batchnorms[self.n_layers - 2](embeddings)))\n",
    "            else:\n",
    "                for i in range(self.n_layers - 2):\n",
    "                    x = self.dropout_p[i](self.activation(self.linears[i](x)))\n",
    "\n",
    "                embeddings = self.linears[self.n_layers - 2](x)\n",
    "                x = self.dropout_p[self.n_layers - 2](\n",
    "                    self.activation(embeddings))\n",
    "\n",
    "            output: torch.Tensor = self.linears[-1](x)\n",
    "            return [embeddings, output]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pom_mpnn_gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from dgl.nn.pytorch import NNConv\n",
    "from dgllife.model.gnn import MPNNGNN\n",
    "\n",
    "\n",
    "class CustomMPNNGNN(MPNNGNN):\n",
    "    \"\"\"\n",
    "    Customized MPNNGNN layer based MPNNGNN layer in dgllife library.\n",
    "\n",
    "    Additional options:\n",
    "    -> toggle for residual in gnn layer\n",
    "    -> choice for message aggregator type\n",
    "\n",
    "    MPNN is introduced in `Neural Message Passing for Quantum Chemistry\n",
    "    <https://arxiv.org/abs/1704.01212>`__.\n",
    "\n",
    "    This class performs message passing in MPNN\n",
    "    and returns the updated node representations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 node_in_feats: int = 50,\n",
    "                 edge_in_feats: int = 50,\n",
    "                 node_out_feats: int = 64,\n",
    "                 edge_hidden_feats: int = 128,\n",
    "                 num_step_message_passing: int = 6,\n",
    "                 residual: bool = True,\n",
    "                 message_aggregator_type: str = 'sum'):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_in_feats: int\n",
    "            Size for the input node features.\n",
    "        node_out_feats: int\n",
    "            Size for the output node representations. Default to 64.\n",
    "        edge_in_feats: int\n",
    "            Size for the input edge features. Default to 128.\n",
    "        edge_hidden_feats: int\n",
    "            Size for the hidden edge representations.\n",
    "        num_step_message_passing: int\n",
    "            Number of message passing steps. Default to 6.\n",
    "        residual: bool\n",
    "            If true, adds residual layer to gnn layer\n",
    "        message_aggregator_type: str\n",
    "            message aggregator type, 'sum', 'mean' or 'max'\n",
    "        \"\"\"\n",
    "        super(CustomMPNNGNN,\n",
    "              self).__init__(node_in_feats=node_in_feats,\n",
    "                             edge_in_feats=edge_in_feats,\n",
    "                             node_out_feats=node_out_feats,\n",
    "                             edge_hidden_feats=edge_hidden_feats,\n",
    "                             num_step_message_passing=num_step_message_passing)\n",
    "\n",
    "        edge_network = nn.Sequential(\n",
    "            nn.Linear(edge_in_feats, edge_hidden_feats), nn.ReLU(),\n",
    "            nn.Linear(edge_hidden_feats, node_out_feats * node_out_feats))\n",
    "        self.gnn_layer = NNConv(in_feats=node_out_feats,\n",
    "                                out_feats=node_out_feats,\n",
    "                                edge_func=edge_network,\n",
    "                                aggregator_type=message_aggregator_type,\n",
    "                                residual=residual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mpnnpom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Union, Optional, Callable, Dict\n",
    "\n",
    "from deepchem.models.losses import Loss, L2Loss\n",
    "from deepchem.models.torch_models.torch_model import TorchModel\n",
    "from deepchem.models.optimizers import Optimizer, LearningRateSchedule\n",
    "\n",
    "# from openpom.layers.pom_ffn import CustomPositionwiseFeedForward\n",
    "# from openpom.utils.loss import CustomMultiLabelLoss\n",
    "# from openpom.utils.optimizer import get_optimizer\n",
    "\n",
    "try:\n",
    "    import dgl\n",
    "    from dgl import DGLGraph\n",
    "    from dgl.nn.pytorch import Set2Set\n",
    "    from openpom.layers.pom_mpnn_gnn import CustomMPNNGNN\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    raise ImportError('This module requires dgl and dgllife')\n",
    "\n",
    "\n",
    "class MPNNPOM(nn.Module):\n",
    "    \"\"\"\n",
    "    MPNN model computes a principal odor map\n",
    "    using multilabel-classification based on the pre-print:\n",
    "    \"A Principal Odor Map Unifies DiverseTasks in Human\n",
    "        Olfactory Perception\" [1]\n",
    "\n",
    "    This model proceeds as follows:\n",
    "\n",
    "    * Combine latest node representations and edge features in\n",
    "        updating node representations, which involves multiple\n",
    "        rounds of message passing.\n",
    "    * For each graph, compute its representation by radius 0 combination\n",
    "        to fold atom and bond embeddings together, followed by\n",
    "        'set2set' or 'global_sum_pooling' readout.\n",
    "    * Perform the final prediction using a feed-forward layer.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Brian K. Lee, Emily J. Mayhew, Benjamin Sanchez-Lengeling,\n",
    "        Jennifer N. Wei, Wesley W. Qian, Kelsie Little, Matthew Andres,\n",
    "        Britney B. Nguyen, Theresa Moloy, Jane K. Parker, Richard C. Gerkin,\n",
    "        Joel D. Mainland, Alexander B. Wiltschko\n",
    "        `A Principal Odor Map Unifies Diverse Tasks\n",
    "        in Human Olfactory Perception preprint\n",
    "        <https://www.biorxiv.org/content/10.1101/2022.09.01.504602v4>`_.\n",
    "\n",
    "    .. [2] Benjamin Sanchez-Lengeling, Jennifer N. Wei, Brian K. Lee,\n",
    "        Richard C. Gerkin, Alán Aspuru-Guzik, Alexander B. Wiltschko\n",
    "        `Machine Learning for Scent:\n",
    "        Learning Generalizable Perceptual Representations\n",
    "        of Small Molecules <https://arxiv.org/abs/1910.10685>`_.\n",
    "\n",
    "    .. [3] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley,\n",
    "        Oriol Vinyals, George E. Dahl.\n",
    "        \"Neural Message Passing for Quantum Chemistry.\" ICML 2017.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This class requires DGL (https://github.com/dmlc/dgl)\n",
    "    and DGL-LifeSci (https://github.com/awslabs/dgl-lifesci)\n",
    "    to be installed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_tasks: int,\n",
    "                 node_out_feats: int = 64,\n",
    "                 edge_hidden_feats: int = 128,\n",
    "                 edge_out_feats: int = 64,\n",
    "                 num_step_message_passing: int = 3,\n",
    "                 mpnn_residual: bool = True,\n",
    "                 message_aggregator_type: str = 'sum',\n",
    "                 mode: str = 'classification',\n",
    "                 number_atom_features: int = 134,\n",
    "                 number_bond_features: int = 6,\n",
    "                 n_classes: int = 1,\n",
    "                 nfeat_name: str = 'x',\n",
    "                 efeat_name: str = 'edge_attr',\n",
    "                 readout_type: str = 'set2set',\n",
    "                 num_step_set2set: int = 6,\n",
    "                 num_layer_set2set: int = 3,\n",
    "                 ffn_hidden_list: List = [300],\n",
    "                 ffn_embeddings: int = 256,\n",
    "                 ffn_activation: str = 'relu',\n",
    "                 ffn_dropout_p: float = 0.0,\n",
    "                 ffn_dropout_at_input_no_act: bool = True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_tasks: int\n",
    "            Number of tasks.\n",
    "        node_out_feats: int\n",
    "            The length of the final node representation vectors\n",
    "            before readout. Default to 64.\n",
    "        edge_hidden_feats: int\n",
    "            The length of the hidden edge representation vectors\n",
    "            for mpnn edge network. Default to 128.\n",
    "        edge_out_feats: int\n",
    "            The length of the final edge representation vectors\n",
    "            before readout. Default to 64.\n",
    "        num_step_message_passing: int\n",
    "            The number of rounds of message passing. Default to 3.\n",
    "        mpnn_residual: bool\n",
    "            If true, adds residual layer to mpnn layer. Default to True.\n",
    "        message_aggregator_type: str\n",
    "            MPNN message aggregator type, 'sum', 'mean' or 'max'.\n",
    "            Default to 'sum'.\n",
    "        mode: str\n",
    "            The model type, 'classification' or 'regression'.\n",
    "            Default to 'classification'.\n",
    "        number_atom_features: int\n",
    "            The length of the initial atom feature vectors. Default to 134.\n",
    "        number_bond_features: int\n",
    "            The length of the initial bond feature vectors. Default to 6.\n",
    "        n_classes: int\n",
    "            The number of classes to predict per task\n",
    "            (only used when ``mode`` is 'classification'). Default to 1.\n",
    "        nfeat_name: str\n",
    "            For an input graph ``g``, the model assumes that it stores\n",
    "            node features in ``g.ndata[nfeat_name]`` and will retrieve\n",
    "            input node features from that. Default to 'x'.\n",
    "        efeat_name: str\n",
    "            For an input graph ``g``, the model assumes that it stores\n",
    "            edge features in ``g.edata[efeat_name]`` and will retrieve\n",
    "            input edge features from that. Default to 'edge_attr'.\n",
    "        readout_type: str\n",
    "            The Readout type, 'set2set' or 'global_sum_pooling'.\n",
    "            Default to 'set2set'.\n",
    "        num_step_set2set: int\n",
    "            Number of steps in set2set readout.\n",
    "            Used if, readout_type == 'set2set'.\n",
    "            Default to 6.\n",
    "        num_layer_set2set: int\n",
    "            Number of layers in set2set readout.\n",
    "            Used if, readout_type == 'set2set'.\n",
    "            Default to 3.\n",
    "        ffn_hidden_list: List\n",
    "            List of sizes of hidden layer in the feed-forward network layer.\n",
    "            Default to [300].\n",
    "        ffn_embeddings: int\n",
    "            Size of penultimate layer in the feed-forward network layer.\n",
    "            This determines the Principal Odor Map dimension.\n",
    "            Default to 256.\n",
    "        ffn_activation: str\n",
    "            Activation function to be used in feed-forward network layer.\n",
    "            Can choose between 'relu' for ReLU, 'leakyrelu' for LeakyReLU,\n",
    "            'prelu' for PReLU, 'tanh' for TanH, 'selu' for SELU,\n",
    "            and 'elu' for ELU.\n",
    "        ffn_dropout_p: float\n",
    "            Dropout probability for the feed-forward network layer.\n",
    "            Default to 0.0\n",
    "        ffn_dropout_at_input_no_act: bool\n",
    "            If true, dropout is applied on the input tensor.\n",
    "            For single layer, it is not passed to an activation function.\n",
    "        \"\"\"\n",
    "        if mode not in ['classification', 'regression']:\n",
    "            raise ValueError(\n",
    "                \"mode must be either 'classification' or 'regression'\")\n",
    "\n",
    "        super(MPNNPOM, self).__init__()\n",
    "\n",
    "        self.n_tasks: int = n_tasks\n",
    "        self.mode: str = mode\n",
    "        self.n_classes: int = n_classes\n",
    "        self.nfeat_name: str = nfeat_name\n",
    "        self.efeat_name: str = efeat_name\n",
    "        self.readout_type: str = readout_type\n",
    "        self.ffn_embeddings: int = ffn_embeddings\n",
    "        self.ffn_activation: str = ffn_activation\n",
    "        self.ffn_dropout_p: float = ffn_dropout_p\n",
    "\n",
    "        if mode == 'classification':\n",
    "            self.ffn_output: int = n_tasks * n_classes\n",
    "        else:\n",
    "            self.ffn_output = n_tasks\n",
    "\n",
    "        self.mpnn: nn.Module = CustomMPNNGNN(\n",
    "            node_in_feats=number_atom_features,\n",
    "            node_out_feats=node_out_feats,\n",
    "            edge_in_feats=number_bond_features,\n",
    "            edge_hidden_feats=edge_hidden_feats,\n",
    "            num_step_message_passing=num_step_message_passing,\n",
    "            residual=mpnn_residual,\n",
    "            message_aggregator_type=message_aggregator_type)\n",
    "\n",
    "        self.project_edge_feats: nn.Module = nn.Sequential(\n",
    "            nn.Linear(number_bond_features, edge_out_feats), nn.ReLU())\n",
    "\n",
    "        if self.readout_type == 'set2set':\n",
    "            self.readout_set2set: nn.Module = Set2Set(\n",
    "                input_dim=node_out_feats + edge_out_feats,\n",
    "                n_iters=num_step_set2set,\n",
    "                n_layers=num_layer_set2set)\n",
    "            ffn_input: int = 2 * (node_out_feats + edge_out_feats)\n",
    "        elif self.readout_type == 'global_sum_pooling':\n",
    "            ffn_input = node_out_feats + edge_out_feats\n",
    "        else:\n",
    "            raise Exception(\"readout_type invalid\")\n",
    "\n",
    "        if ffn_embeddings is not None:\n",
    "            d_hidden_list: List = ffn_hidden_list + [ffn_embeddings]\n",
    "\n",
    "        self.ffn: nn.Module = CustomPositionwiseFeedForward(\n",
    "            d_input=ffn_input,\n",
    "            d_hidden_list=d_hidden_list,\n",
    "            d_output=self.ffn_output,\n",
    "            activation=ffn_activation,\n",
    "            dropout_p=ffn_dropout_p,\n",
    "            dropout_at_input_no_act=ffn_dropout_at_input_no_act)\n",
    "\n",
    "    def _readout(self, g: DGLGraph, node_encodings: torch.Tensor,\n",
    "                 edge_feats: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Method to execute the readout phase.\n",
    "        (compute molecules encodings from atom hidden states)\n",
    "\n",
    "        Readout phase consists of radius 0 combination to fold atom\n",
    "        and bond embeddings together,\n",
    "        followed by:\n",
    "            - a reduce-sum across atoms\n",
    "                if `self.readout_type == 'global_sum_pooling'`\n",
    "            - set2set pooling\n",
    "                if `self.readout_type == 'set2set'`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        g: DGLGraph\n",
    "            A DGLGraph for a batch of graphs.\n",
    "            It stores the node features in\n",
    "            ``dgl_graph.ndata[self.nfeat_name]`` and edge features in\n",
    "            ``dgl_graph.edata[self.efeat_name]``.\n",
    "\n",
    "        node_encodings: torch.Tensor\n",
    "            Tensor containing node hidden states.\n",
    "\n",
    "        edge_feats: torch.Tensor\n",
    "            Tensor containing edge features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        batch_mol_hidden_states: torch.Tensor\n",
    "            Tensor containing batchwise molecule encodings.\n",
    "        \"\"\"\n",
    "\n",
    "        g.ndata['node_emb'] = node_encodings\n",
    "        g.edata['edge_emb'] = self.project_edge_feats(edge_feats)\n",
    "\n",
    "        def message_func(edges) -> Dict:\n",
    "            \"\"\"\n",
    "            The message function to generate messages\n",
    "            along the edges for DGLGraph.send_and_recv()\n",
    "            \"\"\"\n",
    "            src_msg: torch.Tensor = torch.cat(\n",
    "                (edges.src['node_emb'], edges.data['edge_emb']), dim=1)\n",
    "            return {'src_msg': src_msg}\n",
    "\n",
    "        def reduce_func(nodes) -> Dict:\n",
    "            \"\"\"\n",
    "            The reduce function to aggregate the messages\n",
    "            for DGLGraph.send_and_recv()\n",
    "            \"\"\"\n",
    "            src_msg_sum: torch.Tensor = torch.sum(nodes.mailbox['src_msg'],\n",
    "                                                  dim=1)\n",
    "            return {'src_msg_sum': src_msg_sum}\n",
    "\n",
    "        # radius 0 combination to fold atom and bond embeddings together\n",
    "        g.send_and_recv(g.edges(),\n",
    "                        message_func=message_func,\n",
    "                        reduce_func=reduce_func)\n",
    "\n",
    "        if self.readout_type == 'set2set':\n",
    "            batch_mol_hidden_states: torch.Tensor = self.readout_set2set(\n",
    "                g, g.ndata['src_msg_sum'])\n",
    "        elif self.readout_type == 'global_sum_pooling':\n",
    "            batch_mol_hidden_states = dgl.sum_nodes(g, 'src_msg_sum')\n",
    "\n",
    "        # batch_size x (node_out_feats + edge_out_feats)\n",
    "        return batch_mol_hidden_states\n",
    "\n",
    "    def forward(\n",
    "        self, g: tuple[DGLGraph,str] \n",
    "    ) -> Union[tuple[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Foward pass for MPNNPOM class. It also returns embeddings for POM.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        g: DGLGraph\n",
    "            A DGLGraph for a batch of graphs. It stores the node features in\n",
    "            ``dgl_graph.ndata[self.nfeat_name]`` and edge features in\n",
    "            ``dgl_graph.edata[self.efeat_name]``.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Union[tuple[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]\n",
    "            The model output.\n",
    "\n",
    "        * When self.mode = 'regression',\n",
    "            its shape will be ``(dgl_graph.batch_size, self.n_tasks)``.\n",
    "        * When self.mode = 'classification',\n",
    "            the output consists of probabilities for classes.\n",
    "            Its shape will be\n",
    "            ``(dgl_graph.batch_size, self.n_tasks, self.n_classes)``\n",
    "            if self.n_tasks > 1;\n",
    "            its shape will be ``(dgl_graph.batch_size, self.n_classes)``\n",
    "            if self.n_tasks is 1.\n",
    "        \"\"\"\n",
    "        node_feats: torch.Tensor = g[0].ndata[self.nfeat_name]\n",
    "        edge_feats: torch.Tensor = g[0].edata[self.efeat_name]\n",
    "\n",
    "        node_encodings: torch.Tensor = self.mpnn(g[0], node_feats, edge_feats)\n",
    "\n",
    "        molecular_encodings: torch.Tensor = self._readout(\n",
    "            g[0], node_encodings, edge_feats)\n",
    "        if self.readout_type == 'global_sum_pooling':\n",
    "            molecular_encodings = F.softmax(molecular_encodings, dim=1)\n",
    "        # g[1] contains the smiles list for the big DGLGraph data, use the smiles to train 3D rep layer and get the molecular_encodings\n",
    "        smiles_list = g[1]\n",
    "\n",
    "        embeddings: torch.Tensor\n",
    "        out: torch.Tensor\n",
    "        embeddings, out = self.ffn(molecular_encodings)\n",
    "\n",
    "        if self.mode == 'classification':\n",
    "            if self.n_tasks == 1:\n",
    "                logits: torch.Tensor = out.view(-1, self.n_classes)\n",
    "            else:\n",
    "                logits = out.view(-1, self.n_tasks, self.n_classes)\n",
    "            proba: torch.Tensor = F.sigmoid(\n",
    "                logits)  # (batch, n_tasks, classes)\n",
    "            if self.n_classes == 1:\n",
    "                proba = proba.squeeze(-1)  # (batch, n_tasks)\n",
    "            # return proba, logits, embeddings\n",
    "            # get output before sigmoid activation\n",
    "            return proba, logits,embeddings\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class MPNNPOMModel(TorchModel):\n",
    "    \"\"\"\n",
    "    MPNNPOMModel for obtaining a principal odor map\n",
    "    using multilabel-classification based on the pre-print:\n",
    "    \"A Principal Odor Map Unifies DiverseTasks in Human\n",
    "        Olfactory Perception\" [1]\n",
    "\n",
    "    * Combine latest node representations and edge features in\n",
    "        updating node representations, which involves multiple\n",
    "        rounds of message passing.\n",
    "    * For each graph, compute its representation by radius 0 combination\n",
    "        to fold atom and bond embeddings together, followed by\n",
    "        'set2set' or 'global_sum_pooling' readout.\n",
    "    * Perform the final prediction using a feed-forward layer.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Brian K. Lee, Emily J. Mayhew, Benjamin Sanchez-Lengeling,\n",
    "        Jennifer N. Wei, Wesley W. Qian, Kelsie Little, Matthew Andres,\n",
    "        Britney B. Nguyen, Theresa Moloy, Jane K. Parker, Richard C. Gerkin,\n",
    "        Joel D. Mainland, Alexander B. Wiltschko\n",
    "        `A Principal Odor Map Unifies Diverse Tasks\n",
    "        in Human Olfactory Perception preprint\n",
    "        <https://www.biorxiv.org/content/10.1101/2022.09.01.504602v4>`_.\n",
    "\n",
    "    .. [2] Benjamin Sanchez-Lengeling, Jennifer N. Wei, Brian K. Lee,\n",
    "        Richard C. Gerkin, Alán Aspuru-Guzik, Alexander B. Wiltschko\n",
    "        `Machine Learning for Scent:\n",
    "        Learning Generalizable Perceptual Representations\n",
    "        of Small Molecules <https://arxiv.org/abs/1910.10685>`_.\n",
    "\n",
    "    .. [3] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley,\n",
    "        Oriol Vinyals, George E. Dahl.\n",
    "        \"Neural Message Passing for Quantum Chemistry.\" ICML 2017.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This class requires DGL (https://github.com/dmlc/dgl) and DGL-LifeSci\n",
    "    (https://github.com/awslabs/dgl-lifesci) to be installed.\n",
    "\n",
    "    The featurizer used with MPNNPOMModel must produce a Deepchem GraphData\n",
    "    object which should have both 'edge' and 'node' features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_tasks: int,\n",
    "                 class_imbalance_ratio: Optional[List] = None,\n",
    "                 sample_weights: Optional[List] = None,\n",
    "                 loss_aggr_type: str = 'sum',\n",
    "                 learning_rate: Union[float, LearningRateSchedule] = 0.001,\n",
    "                 batch_size: int = 100,\n",
    "                 node_out_feats: int = 64,\n",
    "                 edge_hidden_feats: int = 128,\n",
    "                 edge_out_feats: int = 64,\n",
    "                 num_step_message_passing: int = 3,\n",
    "                 mpnn_residual: bool = True,\n",
    "                 message_aggregator_type: str = 'sum',\n",
    "                 mode: str = 'regression',\n",
    "                 number_atom_features: int = 134,\n",
    "                 number_bond_features: int = 6,\n",
    "                 n_classes: int = 1,\n",
    "                 readout_type: str = 'set2set',\n",
    "                 num_step_set2set: int = 6,\n",
    "                 num_layer_set2set: int = 3,\n",
    "                 ffn_hidden_list: List = [300],\n",
    "                 ffn_embeddings: int = 256,\n",
    "                 ffn_activation: str = 'relu',\n",
    "                 ffn_dropout_p: float = 0.0,\n",
    "                 ffn_dropout_at_input_no_act: bool = True,\n",
    "                 weight_decay: float = 1e-5,\n",
    "                 self_loop: bool = False,\n",
    "                 optimizer_name: str = 'adam',\n",
    "                 device_name: Optional[str] = None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_tasks: int\n",
    "            Number of tasks.\n",
    "        class_imbalance_ratio: Optional[List]\n",
    "            List of imbalance ratios per task.\n",
    "        loss_aggr_type: str\n",
    "            loss aggregation type; 'sum' or 'mean'. Default to 'sum'.\n",
    "            Only applies to CustomMultiLabelLoss for classification\n",
    "        learning_rate: Union[float, LearningRateSchedule]\n",
    "            Learning rate value or scheduler object. Default to 0.001.\n",
    "        batch_size: int\n",
    "            Batch size for training. Default to 100.\n",
    "        node_out_feats: int\n",
    "            The length of the final node representation vectors\n",
    "            before readout. Default to 64.\n",
    "        edge_hidden_feats: int\n",
    "            The length of the hidden edge representation vectors\n",
    "            for mpnn edge network. Default to 128.\n",
    "        edge_out_feats: int\n",
    "            The length of the final edge representation vectors\n",
    "            before readout. Default to 64.\n",
    "        num_step_message_passing: int\n",
    "            The number of rounds of message passing. Default to 3.\n",
    "        mpnn_residual: bool\n",
    "            If true, adds residual layer to mpnn layer. Default to True.\n",
    "        message_aggregator_type: str\n",
    "            MPNN message aggregator type, 'sum', 'mean' or 'max'.\n",
    "            Default to 'sum'.\n",
    "        mode: str\n",
    "            The model type, 'classification' or 'regression'.\n",
    "            Default to 'classification'.\n",
    "        number_atom_features: int\n",
    "            The length of the initial atom feature vectors. Default to 134.\n",
    "        number_bond_features: int\n",
    "            The length of the initial bond feature vectors. Default to 6.\n",
    "        n_classes: int\n",
    "            The number of classes to predict per task\n",
    "            (only used when ``mode`` is 'classification'). Default to 1.\n",
    "        readout_type: str\n",
    "            The Readout type, 'set2set' or 'global_sum_pooling'.\n",
    "            Default to 'set2set'.\n",
    "        num_step_set2set: int\n",
    "            Number of steps in set2set readout.\n",
    "            Used if, readout_type == 'set2set'.\n",
    "            Default to 6.\n",
    "        num_layer_set2set: int\n",
    "            Number of layers in set2set readout.\n",
    "            Used if, readout_type == 'set2set'.\n",
    "            Default to 3.\n",
    "        ffn_hidden_list: List\n",
    "            List of sizes of hidden layer in the feed-forward network layer.\n",
    "            Default to [300].\n",
    "        ffn_embeddings: int\n",
    "            Size of penultimate layer in the feed-forward network layer.\n",
    "            This determines the Principal Odor Map dimension.\n",
    "            Default to 256.\n",
    "        ffn_activation: str\n",
    "            Activation function to be used in feed-forward network layer.\n",
    "            Can choose between 'relu' for ReLU, 'leakyrelu' for LeakyReLU,\n",
    "            'prelu' for PReLU, 'tanh' for TanH, 'selu' for SELU,\n",
    "            and 'elu' for ELU.\n",
    "        ffn_dropout_p: float\n",
    "            Dropout probability for the feed-forward network layer.\n",
    "            Default to 0.0\n",
    "        ffn_dropout_at_input_no_act: bool\n",
    "            If true, dropout is applied on the input tensor.\n",
    "            For single layer, it is not passed to an activation function.\n",
    "        weight_decay: float\n",
    "            weight decay value for L1 and L2 regularization. Default to 1e-5.\n",
    "        self_loop: bool\n",
    "            Whether to add self loops for the nodes, i.e. edges\n",
    "            from nodes to themselves. Generally, an MPNNPOMModel\n",
    "            does not require self loops. Default to False.\n",
    "        optimizer_name: str\n",
    "            Name of optimizer to be used from\n",
    "            [adam, adagrad, adamw, sparseadam, rmsprop, sgd, kfac]\n",
    "            Default to 'adam'.\n",
    "        device_name: Optional[str]\n",
    "            The device on which to run computations. If None, a device is\n",
    "            chosen automatically.\n",
    "        kwargs\n",
    "            This can include any keyword argument of TorchModel.\n",
    "        \"\"\"\n",
    "        model: nn.Module = MPNNPOM(\n",
    "            n_tasks=n_tasks,\n",
    "            node_out_feats=node_out_feats,\n",
    "            edge_hidden_feats=edge_hidden_feats,\n",
    "            edge_out_feats=edge_out_feats,\n",
    "            num_step_message_passing=num_step_message_passing,\n",
    "            mpnn_residual=mpnn_residual,\n",
    "            message_aggregator_type=message_aggregator_type,\n",
    "            mode=mode,\n",
    "            number_atom_features=number_atom_features,\n",
    "            number_bond_features=number_bond_features,\n",
    "            n_classes=n_classes,\n",
    "            readout_type=readout_type,\n",
    "            num_step_set2set=num_step_set2set,\n",
    "            num_layer_set2set=num_layer_set2set,\n",
    "            ffn_hidden_list=ffn_hidden_list,\n",
    "            ffn_embeddings=ffn_embeddings,\n",
    "            ffn_activation=ffn_activation,\n",
    "            ffn_dropout_p=ffn_dropout_p,\n",
    "            ffn_dropout_at_input_no_act=ffn_dropout_at_input_no_act)\n",
    "\n",
    "        if class_imbalance_ratio and (len(class_imbalance_ratio) != n_tasks):\n",
    "            raise Exception(\"size of class_imbalance_ratio \\\n",
    "                            should be equal to n_tasks\")\n",
    "\n",
    "        if mode == 'regression':\n",
    "            loss: Loss = L2Loss()\n",
    "            output_types: List = ['prediction']\n",
    "        else:\n",
    "            loss = CustomMultiLabelLoss(\n",
    "                class_imbalance_ratio=class_imbalance_ratio,\n",
    "                loss_aggr_type=loss_aggr_type,\n",
    "                sample_weights=sample_weights,\n",
    "                device=device_name)\n",
    "            output_types = ['prediction', 'loss', 'embedding']\n",
    "\n",
    "        optimizer: Optimizer = get_optimizer(optimizer_name)\n",
    "        optimizer.learning_rate = learning_rate\n",
    "        if device_name is not None:\n",
    "            device: Optional[torch.device] = torch.device(device_name)\n",
    "        else:\n",
    "            device = None\n",
    "        super(MPNNPOMModel, self).__init__(model,\n",
    "                                           loss=loss,\n",
    "                                           output_types=output_types,\n",
    "                                           optimizer=optimizer,\n",
    "                                           learning_rate=learning_rate,\n",
    "                                           batch_size=batch_size,\n",
    "                                           device=device,\n",
    "                                           **kwargs)\n",
    "\n",
    "        self.weight_decay: float = weight_decay\n",
    "        self._self_loop: bool = self_loop\n",
    "        self.regularization_loss: Callable = self._regularization_loss\n",
    "\n",
    "    def _regularization_loss(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        L1 and L2-norm losses for regularization\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            sum of l1_norm and l2_norm\n",
    "        \"\"\"\n",
    "        l1_regularization: torch.Tensor = torch.tensor(0., requires_grad=True)\n",
    "        l2_regularization: torch.Tensor = torch.tensor(0., requires_grad=True)\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'bias' not in name:\n",
    "                l1_regularization = l1_regularization + torch.norm(param, p=1)\n",
    "                l2_regularization = l2_regularization + torch.norm(param, p=2)\n",
    "        l1_norm: torch.Tensor = self.weight_decay * l1_regularization\n",
    "        l2_norm: torch.Tensor = self.weight_decay * l2_regularization\n",
    "        return l1_norm + l2_norm\n",
    "\n",
    "    def _prepare_batch(\n",
    "        self, batch: Tuple[List, List, List]\n",
    "    ) -> Tuple[DGLGraph, List[torch.Tensor], List[torch.Tensor]]:\n",
    "        \"\"\"Create batch data for MPNN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: Tuple[List, List, List]\n",
    "            The tuple is ``(inputs, labels, weights)``.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        g: DGLGraph\n",
    "            DGLGraph for a batch of graphs.\n",
    "        labels: list of torch.Tensor or None\n",
    "            The graph labels.\n",
    "        weights: list of torch.Tensor or None\n",
    "            The weights for each sample or\n",
    "            sample/task pair converted to torch.Tensor.\n",
    "        \"\"\"\n",
    "        inputs: List\n",
    "        labels: List\n",
    "        weights: List\n",
    "\n",
    "        inputs, labels, weights = batch\n",
    "        smiles = list(x[1] for x in inputs[0])\n",
    "        # print(inputs[0])\n",
    "        # now input is an array of [graphdata, smiles(str)], where smiles is used to point to another dataset\n",
    "        dgl_graphs: List[DGLGraph] = [\n",
    "            graph[0].to_dgl_graph(self_loop=self._self_loop)\n",
    "            for graph in inputs[0]\n",
    "        ]\n",
    "        g: DGLGraph = dgl.batch(dgl_graphs).to(self.device)\n",
    "        _, labels, weights = super(MPNNPOMModel, self)._prepare_batch(\n",
    "            ([], labels, weights))\n",
    "        return (g, smiles), labels, weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of tasks:  138\n"
     ]
    }
   ],
   "source": [
    "TASKS = [\n",
    "'alcoholic', 'aldehydic', 'alliaceous', 'almond', 'amber', 'animal',\n",
    "'anisic', 'apple', 'apricot', 'aromatic', 'balsamic', 'banana', 'beefy',\n",
    "'bergamot', 'berry', 'bitter', 'black currant', 'brandy', 'burnt',\n",
    "'buttery', 'cabbage', 'camphoreous', 'caramellic', 'cedar', 'celery',\n",
    "'chamomile', 'cheesy', 'cherry', 'chocolate', 'cinnamon', 'citrus', 'clean',\n",
    "'clove', 'cocoa', 'coconut', 'coffee', 'cognac', 'cooked', 'cooling',\n",
    "'cortex', 'coumarinic', 'creamy', 'cucumber', 'dairy', 'dry', 'earthy',\n",
    "'ethereal', 'fatty', 'fermented', 'fishy', 'floral', 'fresh', 'fruit skin',\n",
    "'fruity', 'garlic', 'gassy', 'geranium', 'grape', 'grapefruit', 'grassy',\n",
    "'green', 'hawthorn', 'hay', 'hazelnut', 'herbal', 'honey', 'hyacinth',\n",
    "'jasmin', 'juicy', 'ketonic', 'lactonic', 'lavender', 'leafy', 'leathery',\n",
    "'lemon', 'lily', 'malty', 'meaty', 'medicinal', 'melon', 'metallic',\n",
    "'milky', 'mint', 'muguet', 'mushroom', 'musk', 'musty', 'natural', 'nutty',\n",
    "'odorless', 'oily', 'onion', 'orange', 'orangeflower', 'orris', 'ozone',\n",
    "'peach', 'pear', 'phenolic', 'pine', 'pineapple', 'plum', 'popcorn',\n",
    "'potato', 'powdery', 'pungent', 'radish', 'raspberry', 'ripe', 'roasted',\n",
    "'rose', 'rummy', 'sandalwood', 'savory', 'sharp', 'smoky', 'soapy',\n",
    "'solvent', 'sour', 'spicy', 'strawberry', 'sulfurous', 'sweaty', 'sweet',\n",
    "'tea', 'terpenic', 'tobacco', 'tomato', 'tropical', 'vanilla', 'vegetable',\n",
    "'vetiver', 'violet', 'warm', 'waxy', 'weedy', 'winey', 'woody'\n",
    "]\n",
    "print(\"No of tasks: \", len(TASKS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# download curated dataset\n",
    "# !wget https://raw.githubusercontent.com/ARY2260/openpom/main/openpom/data/curated_datasets/curated_GS_LF_merged_4983.csv\n",
    "\n",
    "# The curated dataset can also found at `openpom/data/curated_datasets/curated_GS_LF_merged_4983.csv` in the repo.\n",
    "\n",
    "# input_file = '/home/stephen/openpom/openpom/data/curated_datasets/curated_GS_LF_merged_4983.csv' # or new downloaded file path\n",
    "input_file = '/home/stephen/openpom/openpom/data/curated_datasets/curated_GS_LF_merged_4983.csv' # or new downloaded file path\n",
    "# dataset3d ={}\n",
    "# with open(input_file, mode='r', newline='') as file:\n",
    "#     csv_reader = csv.DictReader(file)\n",
    "#     for row in csv_reader:\n",
    "#         smiles = row['nonStereoSMILES']\n",
    "#         # Parse the coordinates string into a list of lists of floats\n",
    "#         coordinates = [] \n",
    "#         # [list(map(float, coord.strip().split(','))) for coord in row['Coordinates'].split('\\n')]\n",
    "#         for coord in row['Coordinates'].split('\\n'):\n",
    "#             try:\n",
    "#                 coordinates.append(list(map(float,coord.strip().split(','))))\n",
    "#             except:\n",
    "#                 for atom in Chem.MolFromSmiles(smiles).GetAtoms():\n",
    "#                     coordinates.append([0,0,0])\n",
    "            \n",
    "#         # Add the SMILES string as the key and coordinates as the value\n",
    "#         dataset3d[smiles] = coordinates\n",
    "# def getAtom3dInfo(smiles,atomIdx):\n",
    "#     return dataset3d[smiles][atomIdx]\n",
    "# print(len(dataset3d['CC(O)CN']))\n",
    "\n",
    "# print(len(dataset3d['CCCCCCCCC(=O)NCc1ccc(O)c(OC)c1']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "featurizer = GraphFeaturizer()\n",
    "smiles_field = 'nonStereoSMILES'\n",
    "loader = dc.data.CSVLoader(tasks=TASKS,\n",
    "                   feature_field=smiles_field,\n",
    "                   featurizer=featurizer)\n",
    "dataset = loader.create_dataset(inputs=[input_file])\n",
    "n_tasks = len(dataset.tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CC(O)CN'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(n_tasks)\n",
    "dataset.X[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train valid test splits\n",
    "\n",
    "randomstratifiedsplitter = dc.splits.RandomStratifiedSplitter()\n",
    "train_dataset, valid_dataset, test_dataset = randomstratifiedsplitter.train_valid_test_split(dataset, frac_train = 0.984, frac_valid = 0.016, frac_test = 0, seed = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset:  4895\n",
      "valid_dataset:  88\n",
      "test_dataset:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"train_dataset: \", len(train_dataset))\n",
    "print(\"valid_dataset: \", len(valid_dataset))\n",
    "print(\"test_dataset: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# def get_smiles(cid):\n",
    "#     url = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/property/CanonicalSMILES/JSON\"\n",
    "#     response = requests.get(url)\n",
    "#     if response.status_code == 200:\n",
    "#         data = response.json()\n",
    "#         try:\n",
    "#             smiles = data['PropertyTable']['Properties'][0]['CanonicalSMILES']\n",
    "#             print(smiles)\n",
    "#             return smiles\n",
    "#         except (IndexError, KeyError):\n",
    "#             return None\n",
    "#     else:\n",
    "#         return None\n",
    "# intersection_cid_list = [62465, 5634, 7685, 1031, 1032, 31244, 527, 31249, 31252, 15380, 7710, 31265, 7714, 31266, 1060, 4133, 6184, 31272, 31276, 1068, 14896, 7731, 12348, 7749, 12367, 6736, 7762, 61016, 7770, 8797, 1140, 7284, 7799, 1146, 10882, 6276, 10890, 8842, 8857, 1183, 7848, 176, 7344, 177, 180, 22201, 8892, 10430, 3776, 7363, 2758, 17100, 7888, 10448, 61138, 8918, 7895, 26331, 10976, 240, 7921, 3314, 7410, 12020, 11509, 11002, 261, 263, 264, 61199, 798, 7966, 11552, 7460, 22311, 6448, 307, 7991, 2879, 10560, 323, 326, 6989, 24915, 8025, 22873, 8030, 7519, 6501, 91497, 19310, 8048, 22386, 8051, 11124, 20859, 8063, 62336, 9609, 18827, 6544, 12178, 7059, 12180, 6549, 2969, 8091, 7583, 6561, 11173, 6054, 8103, 7590, 6569, 7600, 8118, 6584, 8635, 957, 6590, 8129, 8130, 12741, 454, 12232, 460, 14286, 7119, 7632, 62433, 10722, 997, 999, 62444, 7150, 8186, 7165, 6654]\n",
    "# data = {'cid': intersection_cid_list, 'smiles': [get_smiles(cid) for cid in intersection_cid_list]}\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Save the DataFrame to a new CSV file\n",
    "# output_file_path = 'cids_with_smiles.csv'  # Replace with your desired output file path\n",
    "# df.to_csv(output_file_path, index=False)\n",
    "file_path = '162_with_smiles.csv'  # Replace with your actual file path\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# List of CIDs you are interested in\n",
    "intersection_cid_list = [62465, 5634, 7685, 1031, 1032, 31244, 527, 31249, 31252, 15380, 7710, 31265, 7714, 31266, 1060, 4133, 6184, 31272, 31276, 1068, 7731, 12348, 7749, 12367, 6736, 7762, 61016, 7770, 8797, 612, 1127, 1140, 7284, 7799, 1146, 10882, 6276, 10890, 8842, 8857, 8858, 1183, 7848, 176, 7344, 177, 179, 180, 22201, 8892, 10430, 3776, 7363, 2758, 17100, 7888, 10448, 61138, 8918, 7895, 26331, 10976, 240, 7921, 3314, 7410, 12020, 11509, 11002, 261, 263, 264, 61199, 798, 7966, 11552, 7460, 7463, 307, 7991, 2879, 10560, 323, 326, 6989, 24915, 8025, 22873, 8030, 7519, 6501, 91497, 19310, 8048, 22386, 8051, 11124, 20859, 8063, 62336, 9609, 18827, 6544, 12178, 7059, 12180, 2969, 8091, 7583, 6561, 11173, 6054, 8103, 6569, 7600, 8118, 6584, 8635, 957, 6590, 8129, 8130, 12741, 454, 12232, 460, 14286, 7119, 7632, 62433, 10722, 7654, 999, 62444, 7150, 8186, 7165, 6654, 650]\n",
    "\n",
    "# Filter the DataFrame to only include rows with CIDs in the list\n",
    "filtered_df = df[df['CID'].isin(intersection_cid_list)]\n",
    "\n",
    "# Create a set of SMILES strings from the filtered DataFrame\n",
    "intersection_smiles_set = set(filtered_df['nonStereoSMILES'])\n",
    "for elt in valid_dataset.X[:,1]:\n",
    "    if elt in intersection_cid_list:\n",
    "        print(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratios = get_class_imbalance_ratio(train_dataset)\n",
    "\n",
    "sample_weights = get_sample_weights(train_dataset, intersection_smiles_set)\n",
    "assert len(train_ratios) == n_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = ExponentialDecay(initial_rate=0.001, decay_rate=0.5, decay_steps=32*15, staircase=True)\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "\n",
    "model = MPNNPOMModel(n_tasks = n_tasks,\n",
    "                            batch_size=128,\n",
    "                            learning_rate=learning_rate,\n",
    "                            class_imbalance_ratio = train_ratios,\n",
    "                            sample_weights=sample_weights,\n",
    "                            loss_aggr_type = 'sum',\n",
    "                            node_out_feats = 100,\n",
    "                            edge_hidden_feats = 75,\n",
    "                            edge_out_feats = 100,\n",
    "                            num_step_message_passing = 5,\n",
    "                            mpnn_residual = True,\n",
    "                            message_aggregator_type = 'sum',\n",
    "                            mode = 'classification',\n",
    "                            number_atom_features = GraphConvConstants.ATOM_FDIM,\n",
    "                            number_bond_features = GraphConvConstants.BOND_FDIM,\n",
    "                            n_classes = 1,\n",
    "                            readout_type = 'set2set',\n",
    "                            num_step_set2set = 3,\n",
    "                            num_layer_set2set = 2,\n",
    "                            ffn_hidden_list= [392, 392],\n",
    "                            ffn_embeddings = 256,\n",
    "                            ffn_activation = 'relu',\n",
    "                            ffn_dropout_p = 0.12,\n",
    "                            ffn_dropout_at_input_no_act = False,\n",
    "                            weight_decay = 1e-5,\n",
    "                            self_loop = False,\n",
    "                            optimizer_name = 'adam',\n",
    "                            log_frequency = 32,\n",
    "                            model_dir = './examples/experiments',\n",
    "                            device_name='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/62 ; loss = 3.5446649278913225; train_scores = 0.6615641030279492; valid_scores = 0.6750207066533\n",
      "epoch 2/62 ; loss = 3.403613771711077; train_scores = 0.7614383626863442; valid_scores = 0.749537854255211\n",
      "epoch 3/62 ; loss = 3.355686369396391; train_scores = 0.796350771055595; valid_scores = 0.8005994142277463\n",
      "epoch 4/62 ; loss = 3.286780221121652; train_scores = 0.8165318979458897; valid_scores = 0.798987610950277\n",
      "epoch 5/62 ; loss = 2.654901663462321; train_scores = 0.8212569157806578; valid_scores = 0.813355065731547\n",
      "epoch 6/62 ; loss = 2.9323192596435548; train_scores = 0.8530324564436446; valid_scores = 0.848380044966851\n",
      "epoch 7/62 ; loss = 3.049944484935087; train_scores = 0.8612312968466114; valid_scores = 0.8525586500703933\n",
      "epoch 8/62 ; loss = 3.037635167439779; train_scores = 0.8584524864998806; valid_scores = 0.8443205980565839\n",
      "epoch 9/62 ; loss = 3.0049480315177672; train_scores = 0.8719402762248631; valid_scores = 0.858350429841117\n",
      "epoch 10/62 ; loss = 2.788998285929362; train_scores = 0.8727308976725855; valid_scores = 0.8655941306012124\n",
      "epoch 11/62 ; loss = 2.955563471867488; train_scores = 0.8772361461192751; valid_scores = 0.864220041592274\n",
      "epoch 12/62 ; loss = 2.9010095596313477; train_scores = 0.8778599784805619; valid_scores = 0.8557767989983969\n",
      "epoch 13/62 ; loss = 2.890272917570891; train_scores = 0.885690587912466; valid_scores = 0.865635099159102\n",
      "epoch 14/62 ; loss = 2.032313346862793; train_scores = 0.871787580998017; valid_scores = 0.8471643730796534\n",
      "epoch 15/62 ; loss = 2.6798284318712025; train_scores = 0.8876585338773634; valid_scores = 0.8694530376167201\n",
      "epoch 16/62 ; loss = 2.7914178371429443; train_scores = 0.8864982920845081; valid_scores = 0.8661835435281641\n",
      "epoch 17/62 ; loss = 2.7951235563858696; train_scores = 0.891591181170333; valid_scores = 0.865362471036234\n",
      "epoch 18/62 ; loss = 2.784529622395833; train_scores = 0.8955242462546195; valid_scores = 0.8737100657410728\n",
      "epoch 19/62 ; loss = 2.3966934204101564; train_scores = 0.9006847024835586; valid_scores = 0.8824416262880783\n",
      "epoch 20/62 ; loss = 2.6949793497721353; train_scores = 0.9028175806425244; valid_scores = 0.8725039079944014\n",
      "epoch 21/62 ; loss = 2.7130932055021586; train_scores = 0.9011478649911457; valid_scores = 0.8746643575278447\n",
      "epoch 22/62 ; loss = 2.7311008159930887; train_scores = 0.901607274718011; valid_scores = 0.8689207840170056\n",
      "epoch 23/62 ; loss = 0.9126591682434082; train_scores = 0.9069949244155325; valid_scores = 0.8770820427165918\n",
      "epoch 24/62 ; loss = 2.6136224269866943; train_scores = 0.9035819021507678; valid_scores = 0.8808128801073963\n",
      "epoch 25/62 ; loss = 2.585011291503906; train_scores = 0.9104423582357215; valid_scores = 0.8814900327303363\n",
      "epoch 26/62 ; loss = 2.6197716106068003; train_scores = 0.8990440575994465; valid_scores = 0.8610339183504432\n",
      "epoch 27/62 ; loss = 2.621295797413793; train_scores = 0.9062735282380798; valid_scores = 0.8771592531404743\n",
      "epoch 28/62 ; loss = 2.2370519638061523; train_scores = 0.9118952167046862; valid_scores = 0.8760867876689523\n",
      "epoch 29/62 ; loss = 2.5190799019553443; train_scores = 0.9196516702005265; valid_scores = 0.8817310087857126\n",
      "epoch 30/62 ; loss = 2.538958443535699; train_scores = 0.9200114752116142; valid_scores = 0.885180334736121\n",
      "epoch 31/62 ; loss = 2.558412780761719; train_scores = 0.9212450818601454; valid_scores = 0.8764843712074618\n",
      "epoch 32/62 ; loss = 2.5484719276428223; train_scores = 0.9223702936862223; valid_scores = 0.8852487372167246\n",
      "epoch 33/62 ; loss = 2.330948965890067; train_scores = 0.9216134115716461; valid_scores = 0.8813101854855131\n",
      "epoch 34/62 ; loss = 2.458574022565569; train_scores = 0.9242186620041818; valid_scores = 0.8776261527047005\n",
      "epoch 35/62 ; loss = 2.4300043015252974; train_scores = 0.9258493373245079; valid_scores = 0.8836348116456072\n",
      "epoch 36/62 ; loss = 2.4815123421805247; train_scores = 0.9225892718986834; valid_scores = 0.8791985307674189\n",
      "epoch 37/62 ; loss = 1.999528408050537; train_scores = 0.9277069198874964; valid_scores = 0.8777091868795373\n",
      "epoch 38/62 ; loss = 2.3887319564819336; train_scores = 0.9284220560329466; valid_scores = 0.884794002870978\n",
      "epoch 39/62 ; loss = 2.3728590572581574; train_scores = 0.9300819940233614; valid_scores = 0.884139306064181\n",
      "epoch 40/62 ; loss = 2.3798093795776367; train_scores = 0.9318973491700688; valid_scores = 0.8815593560973253\n",
      "epoch 41/62 ; loss = 2.373904812720514; train_scores = 0.9332834877293243; valid_scores = 0.8738417628020527\n",
      "epoch 42/62 ; loss = 2.173490842183431; train_scores = 0.9363424104483374; valid_scores = 0.8797903422063309\n",
      "epoch 43/62 ; loss = 2.2671629098745494; train_scores = 0.9387646033794907; valid_scores = 0.8833588380173862\n",
      "epoch 44/62 ; loss = 2.2983932495117188; train_scores = 0.9379679787348966; valid_scores = 0.8795333135297814\n",
      "epoch 45/62 ; loss = 2.329281559696904; train_scores = 0.937619702911424; valid_scores = 0.8797885343247972\n",
      "epoch 46/62 ; loss = 1.6981358528137207; train_scores = 0.9416478551383606; valid_scores = 0.8819815897797725\n",
      "epoch 47/62 ; loss = 2.1791341569688587; train_scores = 0.9399411091425703; valid_scores = 0.8847621647517447\n",
      "epoch 48/62 ; loss = 2.2299251556396484; train_scores = 0.9383208541577661; valid_scores = 0.8754740054711297\n",
      "epoch 49/62 ; loss = 2.226335608440897; train_scores = 0.9421570875691411; valid_scores = 0.8760273329072611\n",
      "epoch 50/62 ; loss = 2.2467740376790366; train_scores = 0.9422050131691334; valid_scores = 0.87136778419199\n",
      "epoch 51/62 ; loss = 2.0091981887817383; train_scores = 0.9439552321386742; valid_scores = 0.8720990211380664\n",
      "epoch 52/62 ; loss = 2.148469607035319; train_scores = 0.945128752416516; valid_scores = 0.875529732635754\n",
      "epoch 53/62 ; loss = 2.1382807681435034; train_scores = 0.9432291094128296; valid_scores = 0.872958115475175\n",
      "epoch 54/62 ; loss = 2.1342796912560096; train_scores = 0.9470708853325723; valid_scores = 0.8801237403535078\n",
      "epoch 55/62 ; loss = 0.8444828987121582; train_scores = 0.9499873784260117; valid_scores = 0.8752098585387036\n",
      "epoch 56/62 ; loss = 2.0058085918426514; train_scores = 0.9487989685636241; valid_scores = 0.8699847371537276\n",
      "epoch 57/62 ; loss = 2.0471285502115886; train_scores = 0.9512857836273751; valid_scores = 0.8787983172493447\n",
      "epoch 58/62 ; loss = 2.063984437422319; train_scores = 0.9508218487799773; valid_scores = 0.8722960862010691\n",
      "epoch 59/62 ; loss = 2.0250156007964035; train_scores = 0.951993081092249; valid_scores = 0.869290848021277\n",
      "epoch 60/62 ; loss = 1.8203881978988647; train_scores = 0.9525326532246543; valid_scores = 0.8737630719422322\n",
      "epoch 61/62 ; loss = 2.0099171725186435; train_scores = 0.9529852657049637; valid_scores = 0.8692263475916105\n",
      "epoch 62/62 ; loss = 2.002405802408854; train_scores = 0.9535492166011548; valid_scores = 0.8679877983233194\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "for epoch in range(1, nb_epoch+1):\n",
    "        loss = model.fit(\n",
    "              train_dataset,\n",
    "              nb_epoch=1,\n",
    "              max_checkpoints_to_keep=1,\n",
    "              deterministic=False,\n",
    "              restore=epoch>1)\n",
    "        train_scores = model.evaluate(train_dataset, [metric])['roc_auc_score']\n",
    "        valid_scores = model.evaluate(valid_dataset, [metric])['roc_auc_score']\n",
    "        print(f\"epoch {epoch}/{nb_epoch} ; loss = {loss}; train_scores = {train_scores}; valid_scores = {valid_scores}\")\n",
    "model.save_checkpoint()\n",
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_scores = model.evaluate(test_dataset, [metric])['roc_auc_score']\n",
    "# pred = model.predict(test_dataset)\n",
    "# print(pred.shape)\n",
    "\n",
    "# challenge_dataset_input = \"/home/stephen/openpom/pred_percept_single160.csv\"\n",
    "\n",
    "\n",
    "# # Load the CSV file into a pandas DataFrame\n",
    "# file_path = challenge_dataset_input  # Replace with your actual file path\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Function to get SMILES from CID using PubChem API\n",
    "# def get_smiles_from_cid(cid):\n",
    "#     url = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/property/CanonicalSMILES/JSON\"\n",
    "#     response = requests.get(url)\n",
    "#     if response.status_code == 200:\n",
    "#         data = response.json()\n",
    "#         try:\n",
    "#             smiles = data['PropertyTable']['Properties'][0]['CanonicalSMILES']\n",
    "#             return smiles\n",
    "#         except (IndexError, KeyError):\n",
    "#             return None\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# # Apply the function to each CID in the DataFrame and create a new column for SMILES\n",
    "# # print(df.columns)\n",
    "# df['nonStereoSMILES'] = df['Unnamed: 0'].apply(get_smiles_from_cid)\n",
    "\n",
    "# # Save the DataFrame to a new CSV file\n",
    "output_file_path = '/home/stephen/openpom/162_with_smiles.csv'  # Replace with your desired output file path\n",
    "# df.to_csv(output_file_path, index=False)\n",
    "# print(model.get_model().get_ffn().last2Layer_embeddings()[0].shape)\n",
    "# print(\"time_taken: \", str(end_time-start_time))\n",
    "# print(\"test_score: \", test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of TASKS:  21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "challenge_TASKS = ['INTENSITY','PLEASANTNESS','BAKERY','SWEET','FRUIT','FISH','GARLIC','SPICES','COLD','SOUR','BURNT','ACID',\n",
    "                   'WARM','MUSKY','SWEATY','AMMONIA','DECAYED','WOOD','GRASS','FLOWER','CHEMICAL']\n",
    "print('No. of TASKS: ' , len(challenge_TASKS))\n",
    "\n",
    "featurizer = GraphFeaturizer()\n",
    "smiles_field = 'nonStereoSMILES'\n",
    "cloader = dc.data.CSVLoader(tasks=challenge_TASKS,\n",
    "                   feature_field=smiles_field,\n",
    "                   featurizer=featurizer)\n",
    "challenge_dataset = cloader.create_dataset(inputs=[output_file_path])\n",
    "\n",
    "\n",
    "pred_160 =  model.predict(challenge_dataset)\n",
    "# print(pred_160[0])\n",
    "# print(challenge_dataset.X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "df =  pd.read_csv(output_file_path)\n",
    "\n",
    "df['prediction'] = [json.dumps(row.tolist()) for row in pred_160]\n",
    "output_path = 'output_with_predictions_weighted.csv'  # Replace with your desired output file path\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import deepchem as dc\n",
    "# from openpom.feat.graph_featurizer import GraphFeaturizer, GraphConvConstants\n",
    "# from openpom.utils.data_utils import get_class_imbalance_ratio, IterativeStratifiedSplitter\n",
    "# from openpom.models.mpnn_pom import MPNNPOMModel\n",
    "# from datetime import datetime\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of tasks:  138\n"
     ]
    }
   ],
   "source": [
    "TASKS = [\n",
    "'alcoholic', 'aldehydic', 'alliaceous', 'almond', 'amber', 'animal',\n",
    "'anisic', 'apple', 'apricot', 'aromatic', 'balsamic', 'banana', 'beefy',\n",
    "'bergamot', 'berry', 'bitter', 'black currant', 'brandy', 'burnt',\n",
    "'buttery', 'cabbage', 'camphoreous', 'caramellic', 'cedar', 'celery',\n",
    "'chamomile', 'cheesy', 'cherry', 'chocolate', 'cinnamon', 'citrus', 'clean',\n",
    "'clove', 'cocoa', 'coconut', 'coffee', 'cognac', 'cooked', 'cooling',\n",
    "'cortex', 'coumarinic', 'creamy', 'cucumber', 'dairy', 'dry', 'earthy',\n",
    "'ethereal', 'fatty', 'fermented', 'fishy', 'floral', 'fresh', 'fruit skin',\n",
    "'fruity', 'garlic', 'gassy', 'geranium', 'grape', 'grapefruit', 'grassy',\n",
    "'green', 'hawthorn', 'hay', 'hazelnut', 'herbal', 'honey', 'hyacinth',\n",
    "'jasmin', 'juicy', 'ketonic', 'lactonic', 'lavender', 'leafy', 'leathery',\n",
    "'lemon', 'lily', 'malty', 'meaty', 'medicinal', 'melon', 'metallic',\n",
    "'milky', 'mint', 'muguet', 'mushroom', 'musk', 'musty', 'natural', 'nutty',\n",
    "'odorless', 'oily', 'onion', 'orange', 'orangeflower', 'orris', 'ozone',\n",
    "'peach', 'pear', 'phenolic', 'pine', 'pineapple', 'plum', 'popcorn',\n",
    "'potato', 'powdery', 'pungent', 'radish', 'raspberry', 'ripe', 'roasted',\n",
    "'rose', 'rummy', 'sandalwood', 'savory', 'sharp', 'smoky', 'soapy',\n",
    "'solvent', 'sour', 'spicy', 'strawberry', 'sulfurous', 'sweaty', 'sweet',\n",
    "'tea', 'terpenic', 'tobacco', 'tomato', 'tropical', 'vanilla', 'vegetable',\n",
    "'vetiver', 'violet', 'warm', 'waxy', 'weedy', 'winey', 'woody'\n",
    "]\n",
    "\n",
    "print(\"No of tasks: \", len(TASKS))\n",
    "n_tasks = len(TASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if no splits saved yet\n",
    "\n",
    "# download curated dataset\n",
    "# !wget https://raw.githubusercontent.com/ARY2260/openpom/main/openpom/data/curated_datasets/curated_GS_LF_merged_4983.csv\n",
    "\n",
    "# The curated dataset can also found at `openpom/data/curated_datasets/curated_GS_LF_merged_4983.csv` in the repo.\n",
    "\n",
    "input_file = '/home/stephen/openpom/openpom/data/curated_datasets/curated_GS_LF_merged_4983.csv' # or new downloaded file path\n",
    "\n",
    "# get dataset\n",
    "\n",
    "featurizer = GraphFeaturizer()\n",
    "smiles_field = 'nonStereoSMILES'\n",
    "loader = dc.data.CSVLoader(tasks=TASKS,\n",
    "                   feature_field=smiles_field,\n",
    "                   featurizer=featurizer)\n",
    "dataset = loader.create_dataset(inputs=[input_file])\n",
    "n_tasks = len(dataset.tasks)\n",
    "\n",
    "# get k folds list\n",
    "k = 5\n",
    "splitter = IterativeStratifiedSplitter(order=2)\n",
    "directories = ['']*2*k\n",
    "for fold in range(k):\n",
    "    directories[2 * fold] = f'./ensemble_cv_exp/fold_{fold+1}/train_data'\n",
    "    directories[2 * fold + 1] = f'./ensemble_cv_exp/fold_{fold+1}/cv_data'\n",
    "folds_list = splitter.k_fold_split(dataset=dataset, k=k, directories=directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_ensemble(fold, train_dataset, test_dataset, n_models, nb_epoch,challenge_dataset):\n",
    "    train_ratios = get_class_imbalance_ratio(train_dataset)\n",
    "    sample_weights = get_sample_weights(train_dataset, seed=42)\n",
    "    assert len(train_ratios) == n_tasks\n",
    "\n",
    "    # learning_rate = 0.001\n",
    "    learning_rate = dc.models.optimizers.ExponentialDecay(initial_rate=0.001, decay_rate=0.5, decay_steps=32*20, staircase=True)\n",
    "    metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "\n",
    "    # fit models\n",
    "    for i in tqdm(range(n_models)):\n",
    "        model = MPNNPOMModel(n_tasks = n_tasks,\n",
    "                                batch_size=128,\n",
    "                                learning_rate=learning_rate,\n",
    "                                class_imbalance_ratio = train_ratios,\n",
    "                                loss_aggr_type = 'sum',\n",
    "                                node_out_feats = 100,\n",
    "                                edge_hidden_feats = 75,\n",
    "                                edge_out_feats = 100,\n",
    "                                num_step_message_passing = 5,\n",
    "                                mpnn_residual = True,\n",
    "                                message_aggregator_type = 'sum',\n",
    "                                mode = 'classification',\n",
    "                                number_atom_features = GraphConvConstants.ATOM_FDIM,\n",
    "                                number_bond_features = GraphConvConstants.BOND_FDIM,\n",
    "                                n_classes = 1,\n",
    "                                readout_type = 'set2set',\n",
    "                                num_step_set2set = 3,\n",
    "                                num_layer_set2set = 2,\n",
    "                                ffn_hidden_list= [392, 392],\n",
    "                                ffn_embeddings = 256,\n",
    "                                ffn_activation = 'relu',\n",
    "                                ffn_dropout_p = 0.12,\n",
    "                                ffn_dropout_at_input_no_act = False,\n",
    "                                weight_decay = 1e-5,\n",
    "                                self_loop = False,\n",
    "                                optimizer_name = 'adam',\n",
    "                                log_frequency = 32,\n",
    "                                model_dir = f'./ensemble_cv_exp/ensemble_fold_{fold+1}/experiments_{i+1}',\n",
    "                                device_name='cuda')\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # fit model\n",
    "        loss = model.fit(\n",
    "            train_dataset,\n",
    "            nb_epoch=nb_epoch,\n",
    "            max_checkpoints_to_keep=1,\n",
    "            deterministic=False,\n",
    "            restore=False)\n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        train_scores = model.evaluate(train_dataset, [metric])['roc_auc_score']\n",
    "        test_scores = model.evaluate(test_dataset, [metric])['roc_auc_score']\n",
    "        print(f\"loss = {loss}; train_scores = {train_scores}; test_scores = {test_scores}; time_taken = {str(end_time-start_time)}\")\n",
    "        model.save_checkpoint() # saves final checkpoint => `checkpoint2.pt`\n",
    "        del model\n",
    "        torch.cuda.empty_cache()    \n",
    "\n",
    "    # Get test score from the ensemble\n",
    "    list_preds = []\n",
    "    # challenge predicts\n",
    "    list_cpreds = []\n",
    "    for i in range(n_models):\n",
    "        model = MPNNPOMModel(n_tasks = n_tasks,\n",
    "                                batch_size=128,\n",
    "                                learning_rate=learning_rate,\n",
    "                                class_imbalance_ratio = train_ratios,\n",
    "                                loss_aggr_type = 'sum',\n",
    "                                node_out_feats = 100,\n",
    "                                edge_hidden_feats = 75,\n",
    "                                edge_out_feats = 100,\n",
    "                                num_step_message_passing = 5,\n",
    "                                mpnn_residual = True,\n",
    "                                message_aggregator_type = 'sum',\n",
    "                                mode = 'classification',\n",
    "                                number_atom_features = GraphConvConstants.ATOM_FDIM,\n",
    "                                number_bond_features = GraphConvConstants.BOND_FDIM,\n",
    "                                n_classes = 1,\n",
    "                                readout_type = 'set2set',\n",
    "                                num_step_set2set = 3,\n",
    "                                num_layer_set2set = 2,\n",
    "                                ffn_hidden_list= [392, 392],\n",
    "                                ffn_embeddings = 256,\n",
    "                                ffn_activation = 'relu',\n",
    "                                ffn_dropout_p = 0.12,\n",
    "                                ffn_dropout_at_input_no_act = False,\n",
    "                                weight_decay = 1e-5,\n",
    "                                self_loop = False,\n",
    "                                optimizer_name = 'adam',\n",
    "                                log_frequency = 32,\n",
    "                                model_dir = f'./ensemble_cv_exp/ensemble_fold_{fold+1}/experiments_{i+1}',\n",
    "                                device_name='cuda')\n",
    "        model.restore(f\"./ensemble_cv_exp/ensemble_fold_{fold+1}/experiments_{i+1}/checkpoint2.pt\")\n",
    "        # test_scores = model.evaluate(test_dataset, [metric])['roc_auc_score']\n",
    "        # print(\"test_score: \", test_scores)\n",
    "        preds = model.predict(test_dataset)\n",
    "        cpreds = model.predict(challenge_dataset)\n",
    "        list_preds.append(preds)\n",
    "        list_cpreds.append(cpreds)\n",
    "\n",
    "    preds_arr = np.asarray(list_preds)\n",
    "    cpreds_arr = np.asarray(list_cpreds)\n",
    "    ensemble_preds = np.mean(preds_arr, axis=0)\n",
    "    ensemble_cpreds =  np.mean(cpreds_arr, axis = 0)\n",
    "    return (roc_auc_score(test_dataset.y, ensemble_preds, average=\"macro\"), ensemble_cpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 ensemble starting now.\n",
      "train_dataset:  3963\n",
      "test_dataset:  1020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████████▍                                                                           | 1/10 [01:32<13:54, 92.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6435250043869019; train_scores = 0.9563002086224733; test_scores = 0.8716746449349252; time_taken = 0:01:31.122858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|████████████████▊                                                                   | 2/10 [03:06<12:24, 93.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6794121265411377; train_scores = 0.9545676249378593; test_scores = 0.8735803697589037; time_taken = 0:01:31.666046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|█████████████████████████▏                                                          | 3/10 [04:40<10:54, 93.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.8955579996109009; train_scores = 0.9505791631345845; test_scores = 0.8733754826862802; time_taken = 0:01:32.353650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|█████████████████████████████████▌                                                  | 4/10 [06:12<09:19, 93.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.8951716423034668; train_scores = 0.9528689902341507; test_scores = 0.8701663686689994; time_taken = 0:01:31.057196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████████████████████████                                          | 5/10 [07:46<07:46, 93.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.5795682668685913; train_scores = 0.9575965692024742; test_scores = 0.8689327076087647; time_taken = 0:01:31.844231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 6/10 [09:20<06:14, 93.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.579249620437622; train_scores = 0.95666034663113; test_scores = 0.8687874606897277; time_taken = 0:01:32.218576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|██████████████████████████████████████████████████████████▊                         | 7/10 [10:54<04:41, 93.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.7826263904571533; train_scores = 0.9528823901352466; test_scores = 0.8728401279928477; time_taken = 0:01:32.489951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 8/10 [12:27<03:07, 93.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.7993930578231812; train_scores = 0.9551544645652553; test_scores = 0.8698906797721485; time_taken = 0:01:31.649688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|███████████████████████████████████████████████████████████████████████████▌        | 9/10 [14:01<01:33, 93.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.7229435443878174; train_scores = 0.9517608772015606; test_scores = 0.8754341752248581; time_taken = 0:01:32.771783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [15:36<00:00, 93.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.7918314933776855; train_scores = 0.9541813758083842; test_scores = 0.8718717819038466; time_taken = 0:01:32.994119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|████████████████▍                                                                 | 1/5 [15:40<1:02:40, 940.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 ensemble score:  0.8873141219565731\n",
      "Fold 2 ensemble starting now.\n",
      "train_dataset:  3997\n",
      "test_dataset:  986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████████▍                                                                           | 1/10 [01:38<14:43, 98.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6341204643249512; train_scores = 0.9566379436122779; test_scores = 0.8696190985302328; time_taken = 0:01:36.544928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|████████████████▊                                                                   | 2/10 [03:17<13:08, 98.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.755035161972046; train_scores = 0.9522705567958488; test_scores = 0.8761822711749616; time_taken = 0:01:37.158389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|█████████████████████████▏                                                          | 3/10 [04:57<11:34, 99.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6320730447769165; train_scores = 0.9560556418532109; test_scores = 0.87357930226913; time_taken = 0:01:38.409375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|█████████████████████████████████▌                                                  | 4/10 [06:36<09:54, 99.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.7861170768737793; train_scores = 0.9498253220627413; test_scores = 0.8760226845768484; time_taken = 0:01:37.271020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████████████████████████                                          | 5/10 [08:15<08:16, 99.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.7577598094940186; train_scores = 0.9498350240704226; test_scores = 0.8728842757054582; time_taken = 0:01:37.845969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 6/10 [09:54<06:36, 99.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6499236822128296; train_scores = 0.955466705560324; test_scores = 0.874699185821393; time_taken = 0:01:37.394846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|██████████████████████████████████████████████████████████▊                         | 7/10 [11:32<04:56, 98.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.7711378335952759; train_scores = 0.9499903862630855; test_scores = 0.8729214786366977; time_taken = 0:01:36.381684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 8/10 [13:11<03:17, 98.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6363370418548584; train_scores = 0.9549666481294615; test_scores = 0.87032485397431; time_taken = 0:01:37.201251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|███████████████████████████████████████████████████████████████████████████▌        | 9/10 [14:50<01:38, 98.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.7258200645446777; train_scores = 0.953040548613671; test_scores = 0.8703533379223876; time_taken = 0:01:36.736451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [16:26<00:00, 98.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6715792417526245; train_scores = 0.956166311855072; test_scores = 0.8726977738560912; time_taken = 0:01:35.368046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [32:10<48:28, 969.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 ensemble score:  0.8869184149371335\n",
      "Fold 3 ensemble starting now.\n",
      "train_dataset:  3984\n",
      "test_dataset:  999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████████▍                                                                           | 1/10 [01:36<14:30, 96.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6158984899520874; train_scores = 0.9542197791503823; test_scores = 0.8704401823902974; time_taken = 0:01:34.983657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|████████████████▊                                                                   | 2/10 [03:11<12:46, 95.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.7132742404937744; train_scores = 0.9527399348406904; test_scores = 0.8710960827088176; time_taken = 0:01:33.620875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|█████████████████████████▏                                                          | 3/10 [04:50<11:18, 96.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.5748958587646484; train_scores = 0.9564598694744729; test_scores = 0.8690067837653257; time_taken = 0:01:36.418503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|█████████████████████████████████▌                                                  | 4/10 [06:32<09:55, 99.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6142781972885132; train_scores = 0.9552532496722691; test_scores = 0.8711998132023773; time_taken = 0:01:41.289263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████████████████████████                                          | 5/10 [08:09<08:11, 98.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6607221364974976; train_scores = 0.9537125068464605; test_scores = 0.8700770463476449; time_taken = 0:01:35.085886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 6/10 [09:45<06:30, 97.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6622167825698853; train_scores = 0.954279440466472; test_scores = 0.8718725066821373; time_taken = 0:01:34.562880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|██████████████████████████████████████████████████████████▊                         | 7/10 [11:22<04:51, 97.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.5805692672729492; train_scores = 0.957719359784427; test_scores = 0.8708745750828832; time_taken = 0:01:35.279133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 8/10 [12:59<03:14, 97.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.5349888801574707; train_scores = 0.9589456347422424; test_scores = 0.8702914929496397; time_taken = 0:01:35.240934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|███████████████████████████████████████████████████████████████████████████▌        | 9/10 [14:36<01:37, 97.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.724890947341919; train_scores = 0.9516814180013313; test_scores = 0.875307130330076; time_taken = 0:01:35.437247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [16:15<00:00, 97.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6521098613739014; train_scores = 0.9539120511832755; test_scores = 0.8747763059209586; time_taken = 0:01:37.716822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [48:29<32:27, 973.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 ensemble score:  0.8870904952957991\n",
      "Fold 4 ensemble starting now.\n",
      "train_dataset:  4003\n",
      "test_dataset:  980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████████▍                                                                           | 1/10 [01:36<14:28, 96.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6287542581558228; train_scores = 0.9566525343184855; test_scores = 0.8678340895961376; time_taken = 0:01:34.915587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|████████████████▊                                                                   | 2/10 [03:12<12:49, 96.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6418370008468628; train_scores = 0.956067884502413; test_scores = 0.8718582144604609; time_taken = 0:01:34.232663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|█████████████████████████▏                                                          | 3/10 [04:49<11:17, 96.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6257741451263428; train_scores = 0.9563372157117667; test_scores = 0.8701788440898139; time_taken = 0:01:35.966281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|█████████████████████████████████▌                                                  | 4/10 [06:26<09:40, 96.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.7756750583648682; train_scores = 0.9514639545743597; test_scores = 0.872361831599786; time_taken = 0:01:34.963887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████████████████████████                                          | 5/10 [08:02<08:01, 96.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6584330797195435; train_scores = 0.9546302636960031; test_scores = 0.8695750504085289; time_taken = 0:01:34.154954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 6/10 [09:37<06:24, 96.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.5894982814788818; train_scores = 0.958601357098166; test_scores = 0.8663982520847128; time_taken = 0:01:34.061915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|██████████████████████████████████████████████████████████▊                         | 7/10 [11:13<04:47, 95.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.5524948835372925; train_scores = 0.9586191229373922; test_scores = 0.8679205070636705; time_taken = 0:01:33.871213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 8/10 [12:51<03:13, 96.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6252131462097168; train_scores = 0.9572916312470844; test_scores = 0.8716109537817014; time_taken = 0:01:36.678240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|███████████████████████████████████████████████████████████████████████████▌        | 9/10 [14:34<01:38, 98.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6472058296203613; train_scores = 0.9572436176331647; test_scores = 0.8697011205578145; time_taken = 0:01:40.878415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [16:11<00:00, 97.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.715270757675171; train_scores = 0.9531591507244195; test_scores = 0.8706767801233244; time_taken = 0:01:35.998987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 4/5 [1:04:44<16:14, 974.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 ensemble score:  0.8843187071298421\n",
      "Fold 5 ensemble starting now.\n",
      "train_dataset:  3985\n",
      "test_dataset:  998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████████▍                                                                           | 1/10 [01:34<14:12, 94.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.702138900756836; train_scores = 0.9521387847690915; test_scores = 0.8751401994702805; time_taken = 0:01:32.909623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|████████████████▊                                                                   | 2/10 [03:10<12:40, 95.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6775354146957397; train_scores = 0.9539742744000154; test_scores = 0.8763000567308926; time_taken = 0:01:33.659930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|█████████████████████████▏                                                          | 3/10 [04:44<11:04, 94.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6171324253082275; train_scores = 0.9571025634130835; test_scores = 0.8746976067055074; time_taken = 0:01:33.221295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|█████████████████████████████████▌                                                  | 4/10 [06:26<09:45, 97.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.723649501800537; train_scores = 0.9513927504781439; test_scores = 0.8803128285549635; time_taken = 0:01:40.140731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████████████████████████                                          | 5/10 [08:00<08:01, 96.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.7287044525146484; train_scores = 0.9516555301012892; test_scores = 0.8748088326640047; time_taken = 0:01:32.664946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 6/10 [09:35<06:23, 95.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.730612874031067; train_scores = 0.9518967140907973; test_scores = 0.8795471764806473; time_taken = 0:01:33.198995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|██████████████████████████████████████████████████████████▊                         | 7/10 [11:10<04:46, 95.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.713202953338623; train_scores = 0.9526710647569354; test_scores = 0.8756522977753823; time_taken = 0:01:33.499810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 8/10 [12:45<03:10, 95.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.7112946510314941; train_scores = 0.951269294538907; test_scores = 0.8759483398120069; time_taken = 0:01:33.426687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|███████████████████████████████████████████████████████████████████████████▌        | 9/10 [14:28<01:37, 97.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.5890095233917236; train_scores = 0.9567866520427777; test_scores = 0.8737843073789636; time_taken = 0:01:40.392035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [16:03<00:00, 96.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.71857750415802; train_scores = 0.952078141450469; test_scores = 0.874420710444567; time_taken = 0:01:33.950473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [1:20:51<00:00, 970.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 ensemble score:  0.8908339706789512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_models = 10\n",
    "nb_epoch = 62\n",
    "folds_results = []\n",
    "folds_ensembles = []\n",
    "for fold in tqdm(range(k)):\n",
    "    print(f\"Fold {fold+1} ensemble starting now.\")\n",
    "    train_dataset = dc.data.DiskDataset(directories[2 * fold])\n",
    "    test_dataset = dc.data.DiskDataset(directories[2 * fold + 1])\n",
    "    \n",
    "    print(\"train_dataset: \", len(train_dataset))\n",
    "    print(\"test_dataset: \", len(test_dataset))\n",
    "    fold_result, fold_ensemble= benchmark_ensemble(fold=fold,\n",
    "                                     train_dataset=train_dataset,\n",
    "                                     test_dataset=test_dataset,\n",
    "                                     n_models=n_models,\n",
    "                                     nb_epoch=nb_epoch,challenge_dataset=challenge_dataset)\n",
    "    \n",
    "    print(f\"Fold {fold+1} ensemble score: \", fold_result)\n",
    "    folds_results.append(fold_result)\n",
    "    folds_ensembles.append(fold_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.5910535e+00 -4.2015090e+00 -3.5298297e+00 -2.2822831e+00\n",
      " -2.2648399e+00 -1.2477567e+00 -1.2646991e+00 -4.5026774e+00\n",
      " -4.0482874e+00 -1.1327337e+00 -2.6242426e-01 -4.2799354e+00\n",
      " -2.4552250e+00 -4.0020142e+00 -3.2093384e+00 -2.0431437e+00\n",
      " -3.5969315e+00 -3.9479260e+00  1.2841821e-03 -3.2952323e+00\n",
      " -3.5902710e+00 -1.8022617e+00 -1.1232203e+00 -3.1083984e+00\n",
      " -2.2587631e+00 -3.4375362e+00 -3.3785198e+00 -2.9139245e+00\n",
      " -1.5028133e+00 -1.7282383e+00 -3.5096123e+00 -2.9601483e+00\n",
      "  2.6644370e-01 -2.3787553e+00 -2.0670533e+00 -1.5703940e+00\n",
      " -4.2702436e+00 -2.8365774e+00 -2.3062983e+00 -3.0165160e+00\n",
      " -2.2590470e+00 -9.4517040e-01 -4.1948891e+00 -1.8989738e+00\n",
      " -1.0459563e+00 -1.1468416e+00 -2.9295864e+00 -3.2843940e+00\n",
      " -3.3001697e+00 -3.0441067e+00 -2.1865180e+00 -2.5898354e+00\n",
      " -3.9308884e+00 -2.7995763e+00 -3.4049048e+00 -2.7780671e+00\n",
      " -3.5422177e+00 -4.4791026e+00 -4.3882079e+00 -3.4213996e+00\n",
      " -3.2012286e+00 -2.0092456e+00 -1.6903414e+00 -2.8125067e+00\n",
      " -1.5153580e+00 -2.3022447e+00 -3.5735574e+00 -4.0189142e+00\n",
      " -3.8435853e+00 -3.6939144e+00 -2.7969213e+00 -3.2710114e+00\n",
      " -3.3458562e+00 -4.3482941e-01 -2.8753459e+00 -3.8766789e+00\n",
      " -2.1384296e+00 -1.1870826e+00  4.3918228e-01 -3.8946884e+00\n",
      " -2.3722818e+00 -1.9323429e+00 -2.5041492e+00 -4.1031733e+00\n",
      " -3.5074539e+00 -3.2152023e+00 -8.3590972e-01 -2.7448173e+00\n",
      " -1.3628191e+00 -3.1241844e+00 -2.8926158e+00 -3.1601481e+00\n",
      " -3.8351071e+00 -3.8843675e+00 -2.2146866e+00 -3.2061470e+00\n",
      " -4.1530080e+00 -4.0173430e+00  2.0417781e+00 -2.9501214e+00\n",
      " -3.9974437e+00 -3.9278324e+00 -3.7069821e+00 -3.2344387e+00\n",
      " -6.3688862e-01 -1.2447516e+00 -3.0211911e+00 -3.2969546e+00\n",
      " -3.2459748e+00 -1.4143711e+00 -3.4073148e+00 -2.3123798e+00\n",
      " -2.6402071e+00 -1.3929867e+00 -1.1935675e+00  1.2564627e+00\n",
      " -3.5883591e+00 -2.3199902e+00 -2.8889890e+00  1.5284631e+00\n",
      " -3.0253482e+00 -3.3195019e+00 -3.4224696e+00  1.2754275e+00\n",
      " -1.7436707e+00 -2.7081573e+00 -1.4408143e+00 -3.8447661e+00\n",
      " -3.3017106e+00  5.7935303e-01 -2.5000589e+00 -2.6180599e+00\n",
      " -3.8411553e+00 -8.3543348e-01 -3.2110932e+00 -2.8188446e+00\n",
      " -3.5857182e+00  6.4432514e-01]\n",
      "0.8873\n"
     ]
    }
   ],
   "source": [
    "cv_mean_result = np.mean(folds_results)\n",
    "cv_mean_ensemble_embedding = np.mean(folds_ensembles, axis = 0)\n",
    "print(cv_mean_ensemble_embedding[0])\n",
    "print(round(cv_mean_result, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "df =  pd.read_csv(output_file_path)\n",
    "\n",
    "df['prediction'] = [json.dumps(row.tolist()) for row in cv_mean_ensemble_embedding]\n",
    "output_path = 'ensemble_embeddings.csv'  # Replace with your desired output file path\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
